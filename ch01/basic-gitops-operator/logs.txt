* 
* ==> Audit <==
* |----------------|--------------------------------|----------|---------------|---------|-------------------------------|-------------------------------|
|    Command     |              Args              | Profile  |     User      | Version |          Start Time           |           End Time            |
|----------------|--------------------------------|----------|---------------|---------|-------------------------------|-------------------------------|
| update-check   |                                | minikube | ahmedabdo2070 | v1.21.0 | Sat, 04 Sep 2021 14:55:58 +04 | Sat, 04 Sep 2021 14:55:58 +04 |
| -h             |                                | minikube | ahmedabdo2070 | v1.23.0 | Sun, 12 Sep 2021 20:13:44 +04 | Sun, 12 Sep 2021 20:13:44 +04 |
| delete         |                                | minikube | ahmedabdo2070 | v1.23.0 | Sun, 12 Sep 2021 20:13:52 +04 | Sun, 12 Sep 2021 20:13:53 +04 |
| update-check   |                                | minikube | ahmedabdo2070 | v1.23.0 | Wed, 13 Jul 2022 22:09:49 +04 | Wed, 13 Jul 2022 22:09:49 +04 |
| update-check   |                                | minikube | ahmedabdo2070 | v1.26.0 | 28 Jul 22 23:00 +04           | 28 Jul 22 23:00 +04           |
| update-check   |                                | minikube | ahmedabdo2070 | v1.26.0 | 31 Jul 22 15:14 +04           | 31 Jul 22 15:14 +04           |
| update-check   |                                | minikube | ahmedabdo2070 | v1.26.0 | 06 Aug 22 12:13 +04           | 06 Aug 22 12:13 +04           |
| update-check   |                                | minikube | ahmedabdo2070 | v1.26.0 | 07 Aug 22 19:36 +04           | 07 Aug 22 19:36 +04           |
| update-check   |                                | minikube | ahmedabdo2070 | v1.26.0 | 14 Aug 22 14:18 +04           | 14 Aug 22 14:18 +04           |
| update-check   |                                | minikube | ahmedabdo2070 | v1.26.1 | 20 Aug 22 10:03 +04           | 20 Aug 22 10:03 +04           |
| update-check   |                                | minikube | ahmedabdo2070 | v1.26.1 | 21 Aug 22 12:43 +04           | 21 Aug 22 12:43 +04           |
| update-check   |                                | minikube | ahmedabdo2070 | v1.26.1 | 23 Aug 22 21:47 +04           | 23 Aug 22 21:47 +04           |
| update-check   |                                | minikube | ahmedabdo2070 | v1.26.1 | 06 Mar 23 14:08 +04           | 06 Mar 23 14:08 +04           |
| update-check   |                                | minikube | ahmedabdo2070 | v1.29.0 | 18 Mar 23 12:22 +04           | 18 Mar 23 12:22 +04           |
| update-check   |                                | minikube | ahmedabdo2070 | v1.29.0 | 19 Mar 23 17:26 +04           | 19 Mar 23 17:26 +04           |
| update-check   |                                | minikube | ahmedabdo2070 | v1.29.0 | 30 Jul 23 19:21 +04           | 30 Jul 23 19:21 +04           |
| start          | --driver=virtualbox --memory   | minikube | ahmedabdo2070 | v1.32.0 | 31 Dec 23 18:37 +04           |                               |
|                | 12g                            |          |               |         |                               |                               |
| ssh            |                                | minikube | ahmedabdo2070 | v1.32.0 | 31 Dec 23 18:39 +04           |                               |
| delete         |                                | minikube | ahmedabdo2070 | v1.32.0 | 31 Dec 23 18:39 +04           | 31 Dec 23 18:39 +04           |
| start          | --driver=virtualbox --memory   | minikube | ahmedabdo2070 | v1.32.0 | 31 Dec 23 18:41 +04           |                               |
|                | 12g                            |          |               |         |                               |                               |
| delete         |                                | minikube | ahmedabdo2070 | v1.32.0 | 31 Dec 23 18:42 +04           | 31 Dec 23 18:42 +04           |
| start          | --driver=hyperkit --memory 12g | minikube | ahmedabdo2070 | v1.32.0 | 31 Dec 23 18:42 +04           | 31 Dec 23 18:43 +04           |
| kubectl        | -- get pods -A                 | minikube | ahmedabdo2070 | v1.32.0 | 31 Dec 23 18:44 +04           | 31 Dec 23 18:45 +04           |
| kubectl        | -- get nodes                   | minikube | ahmedabdo2070 | v1.32.0 | 31 Dec 23 18:45 +04           | 31 Dec 23 18:45 +04           |
| help           |                                | minikube | ahmedabdo2070 | v1.32.0 | 31 Dec 23 18:48 +04           | 31 Dec 23 18:48 +04           |
| stop           |                                | minikube | ahmedabdo2070 | v1.32.0 | 31 Dec 23 18:48 +04           | 31 Dec 23 18:48 +04           |
| start          |                                | minikube | ahmedabdo2070 | v1.32.0 | 01 Jan 24 14:20 +04           | 01 Jan 24 14:21 +04           |
| kubectl        | -- get nodes                   | minikube | ahmedabdo2070 | v1.32.0 | 01 Jan 24 14:21 +04           | 01 Jan 24 14:21 +04           |
| kubectl        | -- get clusterrolebindings     | minikube | ahmedabdo2070 | v1.32.0 | 01 Jan 24 14:21 +04           | 01 Jan 24 14:21 +04           |
| kubectl        | -- get resources               | minikube | ahmedabdo2070 | v1.32.0 | 01 Jan 24 14:21 +04           |                               |
| kubectl        | -- get api                     | minikube | ahmedabdo2070 | v1.32.0 | 01 Jan 24 14:21 +04           |                               |
| kubectl        | -- get nodes                   | minikube | ahmedabdo2070 | v1.32.0 | 01 Jan 24 14:21 +04           | 01 Jan 24 14:21 +04           |
| update-context |                                | minikube | ahmedabdo2070 | v1.32.0 | 01 Jan 24 14:24 +04           | 01 Jan 24 14:24 +04           |
| stop           |                                | minikube | ahmedabdo2070 | v1.32.0 | 01 Jan 24 15:43 +04           | 01 Jan 24 15:43 +04           |
| delete         |                                | minikube | ahmedabdo2070 | v1.32.0 | 01 Jan 24 15:43 +04           | 01 Jan 24 15:43 +04           |
| start          | --driver=hyperkit --memory 8g  | minikube | ahmedabdo2070 | v1.32.0 | 01 Jan 24 15:44 +04           |                               |
|                | --nodes 2                      |          |               |         |                               |                               |
|----------------|--------------------------------|----------|---------------|---------|-------------------------------|-------------------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/01/01 15:44:01
Running on machine: AhAbdo-MBP
Binary: Built with gc go1.21.4 for darwin/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0101 15:44:01.220961   61932 out.go:296] Setting OutFile to fd 1 ...
I0101 15:44:01.221238   61932 out.go:348] isatty.IsTerminal(1) = true
I0101 15:44:01.221243   61932 out.go:309] Setting ErrFile to fd 2...
I0101 15:44:01.221247   61932 out.go:348] isatty.IsTerminal(2) = true
I0101 15:44:01.221535   61932 root.go:338] Updating PATH: /Users/ahmedabdo2070/.minikube/bin
I0101 15:44:01.222563   61932 out.go:303] Setting JSON to false
I0101 15:44:01.266125   61932 start.go:128] hostinfo: {"hostname":"AhAbdo-MBP","uptime":791392,"bootTime":1703318049,"procs":712,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"14.2.1","kernelVersion":"23.2.0","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"c861f249-55f6-50a3-8c53-cb9637a9f425"}
W0101 15:44:01.266232   61932 start.go:136] gopshost.Virtualization returned error: not implemented yet
I0101 15:44:01.286305   61932 out.go:177] üòÑ  minikube v1.32.0 on Darwin 14.2.1
I0101 15:44:01.326280   61932 notify.go:220] Checking for updates...
I0101 15:44:01.326766   61932 driver.go:378] Setting default libvirt URI to qemu:///system
I0101 15:44:01.363486   61932 out.go:177] ‚ú®  Using the hyperkit driver based on user configuration
I0101 15:44:01.383375   61932 start.go:298] selected driver: hyperkit
I0101 15:44:01.383397   61932 start.go:902] validating driver "hyperkit" against <nil>
I0101 15:44:01.383418   61932 start.go:913] status for hyperkit: {Installed:true Healthy:true Running:true NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0101 15:44:01.384026   61932 install.go:52] acquiring lock: {Name:mk4023283b30b374c3f04c8805d539e68824c0b8 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0101 15:44:01.384265   61932 install.go:117] Validating docker-machine-driver-hyperkit, PATH=/Users/ahmedabdo2070/.minikube/bin:/Users/ahmedabdo2070/Downloads/cks/google-cloud-sdk/bin:/Library/Frameworks/Python.framework/Versions/3.10/bin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/usr/local/share/dotnet:~/.dotnet/tools:/usr/local/go/bin:/Library/Frameworks/Mono.framework/Versions/Current/Commands:/Users/ahmedabdo2070/.antigen/bundles/robbyrussell/oh-my-zsh/lib:/Users/ahmedabdo2070/.antigen/bundles/robbyrussell/oh-my-zsh/plugins/git:/Users/ahmedabdo2070/.antigen/bundles/robbyrussell/oh-my-zsh/plugins/heroku:/Users/ahmedabdo2070/.antigen/bundles/robbyrussell/oh-my-zsh/plugins/pip:/Users/ahmedabdo2070/.antigen/bundles/robbyrussell/oh-my-zsh/plugins/lein:/Users/ahmedabdo2070/.antigen/bundles/robbyrussell/oh-my-zsh/plugins/command-not-found:/Users/ahmedabdo2070/.antigen/bundles/zsh-users/zsh-autosuggestions:/Users/ahmedabdo2070/.antigen/bundles/zsh-users/zsh-completions:/Users/ahmedabdo2070/.antigen/bundles/zsh-users/zsh-history-substring-search:/Users/ahmedabdo2070/.antigen/bundles/dbz/kube-aliases:/Users/ahmedabdo2070/.antigen/bundles/zsh-users/zsh-syntax-highlighting
I0101 15:44:01.401247   61932 install.go:137] /Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit version is 1.32.0
I0101 15:44:01.413476   61932 install.go:79] stdout: /Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit
I0101 15:44:01.413503   61932 install.go:81] /Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit looks good
I0101 15:44:01.413555   61932 start_flags.go:309] no existing cluster config was found, will generate one from the flags 
I0101 15:44:01.415589   61932 start_flags.go:913] Wait components to verify : map[apiserver:true system_pods:true]
I0101 15:44:01.415678   61932 cni.go:84] Creating CNI manager for ""
I0101 15:44:01.415686   61932 cni.go:136] 0 nodes found, recommending kindnet
I0101 15:44:01.415692   61932 start_flags.go:318] Found "CNI" CNI - setting NetworkPlugin=cni
I0101 15:44:01.415707   61932 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:8192 CPUs:2 DiskSize:20000 VMDriver: Driver:hyperkit HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0101 15:44:01.415902   61932 iso.go:125] acquiring lock: {Name:mk427acb4ed164c27c29eee364135b05eb9b69b7 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0101 15:44:01.434622   61932 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I0101 15:44:01.454378   61932 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0101 15:44:01.454546   61932 preload.go:148] Found local preload: /Users/ahmedabdo2070/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I0101 15:44:01.454607   61932 cache.go:56] Caching tarball of preloaded images
I0101 15:44:01.455052   61932 preload.go:174] Found /Users/ahmedabdo2070/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0101 15:44:01.455090   61932 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0101 15:44:01.455939   61932 profile.go:148] Saving config to /Users/ahmedabdo2070/.minikube/profiles/minikube/config.json ...
I0101 15:44:01.455988   61932 lock.go:35] WriteFile acquiring /Users/ahmedabdo2070/.minikube/profiles/minikube/config.json: {Name:mka4adc1f28fe912d206320c8c6924336333ab14 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0101 15:44:01.458014   61932 start.go:365] acquiring machines lock for minikube: {Name:mk750fdd79c85fae9841895e2430443f1c3daa91 Clock:{} Delay:500ms Timeout:13m0s Cancel:<nil>}
I0101 15:44:01.458260   61932 start.go:369] acquired machines lock for "minikube" in 191.439¬µs
I0101 15:44:01.458310   61932 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.32.1-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:8192 CPUs:2 DiskSize:20000 VMDriver: Driver:hyperkit HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:} &{Name: IP: Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0101 15:44:01.458474   61932 start.go:125] createHost starting for "" (driver="hyperkit")
I0101 15:44:01.477333   61932 out.go:204] üî•  Creating hyperkit VM (CPUs=2, Memory=8192MB, Disk=20000MB) ...
I0101 15:44:01.478906   61932 main.go:141] libmachine: Found binary path at /Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit
I0101 15:44:01.479056   61932 main.go:141] libmachine: Launching plugin server for driver hyperkit
I0101 15:44:01.499693   61932 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:55015
I0101 15:44:01.500403   61932 main.go:141] libmachine: () Calling .GetVersion
I0101 15:44:01.501393   61932 main.go:141] libmachine: Using API Version  1
I0101 15:44:01.501413   61932 main.go:141] libmachine: () Calling .SetConfigRaw
I0101 15:44:01.501867   61932 main.go:141] libmachine: () Calling .GetMachineName
I0101 15:44:01.502092   61932 main.go:141] libmachine: (minikube) Calling .GetMachineName
I0101 15:44:01.502264   61932 main.go:141] libmachine: (minikube) Calling .DriverName
I0101 15:44:01.502446   61932 start.go:159] libmachine.API.Create for "minikube" (driver="hyperkit")
I0101 15:44:01.502496   61932 client.go:168] LocalClient.Create starting
I0101 15:44:01.502547   61932 main.go:141] libmachine: Reading certificate data from /Users/ahmedabdo2070/.minikube/certs/ca.pem
I0101 15:44:01.502622   61932 main.go:141] libmachine: Decoding PEM data...
I0101 15:44:01.502671   61932 main.go:141] libmachine: Parsing certificate...
I0101 15:44:01.502772   61932 main.go:141] libmachine: Reading certificate data from /Users/ahmedabdo2070/.minikube/certs/cert.pem
I0101 15:44:01.502833   61932 main.go:141] libmachine: Decoding PEM data...
I0101 15:44:01.502842   61932 main.go:141] libmachine: Parsing certificate...
I0101 15:44:01.502898   61932 main.go:141] libmachine: Running pre-create checks...
I0101 15:44:01.502910   61932 main.go:141] libmachine: (minikube) Calling .PreCreateCheck
I0101 15:44:01.503089   61932 main.go:141] libmachine: (minikube) DBG | exe=/Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0101 15:44:01.503443   61932 main.go:141] libmachine: (minikube) Calling .GetConfigRaw
I0101 15:44:01.504350   61932 main.go:141] libmachine: Creating machine...
I0101 15:44:01.504363   61932 main.go:141] libmachine: (minikube) Calling .Create
I0101 15:44:01.504493   61932 main.go:141] libmachine: (minikube) DBG | exe=/Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0101 15:44:01.504745   61932 main.go:141] libmachine: (minikube) DBG | I0101 15:44:01.504490   61939 common.go:145] Making disk image using store path: /Users/ahmedabdo2070/.minikube
I0101 15:44:01.504839   61932 main.go:141] libmachine: (minikube) Downloading /Users/ahmedabdo2070/.minikube/cache/boot2docker.iso from file:///Users/ahmedabdo2070/.minikube/cache/iso/amd64/minikube-v1.32.1-amd64.iso...
I0101 15:44:01.774760   61932 main.go:141] libmachine: (minikube) DBG | I0101 15:44:01.774660   61939 common.go:152] Creating ssh key: /Users/ahmedabdo2070/.minikube/machines/minikube/id_rsa...
I0101 15:44:01.925334   61932 main.go:141] libmachine: (minikube) DBG | I0101 15:44:01.925189   61939 common.go:158] Creating raw disk image: /Users/ahmedabdo2070/.minikube/machines/minikube/minikube.rawdisk...
I0101 15:44:01.925343   61932 main.go:141] libmachine: (minikube) DBG | Writing magic tar header
I0101 15:44:01.925351   61932 main.go:141] libmachine: (minikube) DBG | Writing SSH key tar header
I0101 15:44:01.926514   61932 main.go:141] libmachine: (minikube) DBG | I0101 15:44:01.926356   61939 common.go:172] Fixing permissions on /Users/ahmedabdo2070/.minikube/machines/minikube ...
I0101 15:44:02.290078   61932 main.go:141] libmachine: (minikube) DBG | exe=/Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0101 15:44:02.290099   61932 main.go:141] libmachine: (minikube) DBG | clean start, hyperkit pid file doesn't exist: /Users/ahmedabdo2070/.minikube/machines/minikube/hyperkit.pid
I0101 15:44:02.290144   61932 main.go:141] libmachine: (minikube) DBG | Using UUID 0eae74cc-a89b-11ee-8a75-acde48001122
I0101 15:44:02.494807   61932 main.go:141] libmachine: (minikube) DBG | Generated MAC 86:d7:9a:d6:bc:19
I0101 15:44:02.494850   61932 main.go:141] libmachine: (minikube) DBG | Starting with cmdline: loglevel=3 console=ttyS0 console=tty0 noembed nomodeset norestore waitusb=10 systemd.legacy_systemd_cgroup_controller=yes random.trust_cpu=on hw_rng_model=virtio base host=minikube
I0101 15:44:02.494898   61932 main.go:141] libmachine: (minikube) DBG | 2024/01/01 15:44:02 DEBUG: hyperkit: Start &hyperkit.HyperKit{HyperKit:"/usr/local/bin/hyperkit", Argv0:"", StateDir:"/Users/ahmedabdo2070/.minikube/machines/minikube", VPNKitSock:"", VPNKitUUID:"", VPNKitPreferredIPv4:"", UUID:"0eae74cc-a89b-11ee-8a75-acde48001122", Disks:[]hyperkit.Disk{(*hyperkit.RawDisk)(0xc000208270)}, ISOImages:[]string{"/Users/ahmedabdo2070/.minikube/machines/minikube/boot2docker.iso"}, VSock:false, VSockDir:"", VSockPorts:[]int(nil), VSockGuestCID:3, VMNet:true, Sockets9P:[]hyperkit.Socket9P(nil), Kernel:"/Users/ahmedabdo2070/.minikube/machines/minikube/bzimage", Initrd:"/Users/ahmedabdo2070/.minikube/machines/minikube/initrd", Bootrom:"", CPUs:2, Memory:8192, Console:1, Serials:[]hyperkit.Serial(nil), Pid:0, Arguments:[]string(nil), CmdLine:"", process:(*os.Process)(nil)}
I0101 15:44:02.494932   61932 main.go:141] libmachine: (minikube) DBG | 2024/01/01 15:44:02 DEBUG: hyperkit: check &hyperkit.HyperKit{HyperKit:"/usr/local/bin/hyperkit", Argv0:"", StateDir:"/Users/ahmedabdo2070/.minikube/machines/minikube", VPNKitSock:"", VPNKitUUID:"", VPNKitPreferredIPv4:"", UUID:"0eae74cc-a89b-11ee-8a75-acde48001122", Disks:[]hyperkit.Disk{(*hyperkit.RawDisk)(0xc000208270)}, ISOImages:[]string{"/Users/ahmedabdo2070/.minikube/machines/minikube/boot2docker.iso"}, VSock:false, VSockDir:"", VSockPorts:[]int(nil), VSockGuestCID:3, VMNet:true, Sockets9P:[]hyperkit.Socket9P(nil), Kernel:"/Users/ahmedabdo2070/.minikube/machines/minikube/bzimage", Initrd:"/Users/ahmedabdo2070/.minikube/machines/minikube/initrd", Bootrom:"", CPUs:2, Memory:8192, Console:1, Serials:[]hyperkit.Serial(nil), Pid:0, Arguments:[]string(nil), CmdLine:"", process:(*os.Process)(nil)}
I0101 15:44:02.495042   61932 main.go:141] libmachine: (minikube) DBG | 2024/01/01 15:44:02 DEBUG: hyperkit: Arguments: []string{"-A", "-u", "-F", "/Users/ahmedabdo2070/.minikube/machines/minikube/hyperkit.pid", "-c", "2", "-m", "8192M", "-s", "0:0,hostbridge", "-s", "31,lpc", "-s", "1:0,virtio-net", "-U", "0eae74cc-a89b-11ee-8a75-acde48001122", "-s", "2:0,virtio-blk,/Users/ahmedabdo2070/.minikube/machines/minikube/minikube.rawdisk", "-s", "3,ahci-cd,/Users/ahmedabdo2070/.minikube/machines/minikube/boot2docker.iso", "-s", "4,virtio-rnd", "-l", "com1,autopty=/Users/ahmedabdo2070/.minikube/machines/minikube/tty,log=/Users/ahmedabdo2070/.minikube/machines/minikube/console-ring", "-f", "kexec,/Users/ahmedabdo2070/.minikube/machines/minikube/bzimage,/Users/ahmedabdo2070/.minikube/machines/minikube/initrd,earlyprintk=serial loglevel=3 console=ttyS0 console=tty0 noembed nomodeset norestore waitusb=10 systemd.legacy_systemd_cgroup_controller=yes random.trust_cpu=on hw_rng_model=virtio base host=minikube"}
I0101 15:44:02.495065   61932 main.go:141] libmachine: (minikube) DBG | 2024/01/01 15:44:02 DEBUG: hyperkit: CmdLine: "/usr/local/bin/hyperkit -A -u -F /Users/ahmedabdo2070/.minikube/machines/minikube/hyperkit.pid -c 2 -m 8192M -s 0:0,hostbridge -s 31,lpc -s 1:0,virtio-net -U 0eae74cc-a89b-11ee-8a75-acde48001122 -s 2:0,virtio-blk,/Users/ahmedabdo2070/.minikube/machines/minikube/minikube.rawdisk -s 3,ahci-cd,/Users/ahmedabdo2070/.minikube/machines/minikube/boot2docker.iso -s 4,virtio-rnd -l com1,autopty=/Users/ahmedabdo2070/.minikube/machines/minikube/tty,log=/Users/ahmedabdo2070/.minikube/machines/minikube/console-ring -f kexec,/Users/ahmedabdo2070/.minikube/machines/minikube/bzimage,/Users/ahmedabdo2070/.minikube/machines/minikube/initrd,earlyprintk=serial loglevel=3 console=ttyS0 console=tty0 noembed nomodeset norestore waitusb=10 systemd.legacy_systemd_cgroup_controller=yes random.trust_cpu=on hw_rng_model=virtio base host=minikube"
I0101 15:44:02.495086   61932 main.go:141] libmachine: (minikube) DBG | 2024/01/01 15:44:02 DEBUG: hyperkit: Redirecting stdout/stderr to logger
I0101 15:44:02.499155   61932 main.go:141] libmachine: (minikube) DBG | 2024/01/01 15:44:02 DEBUG: hyperkit: Pid is 61943
I0101 15:44:02.499723   61932 main.go:141] libmachine: (minikube) DBG | Attempt 0
I0101 15:44:02.499735   61932 main.go:141] libmachine: (minikube) DBG | exe=/Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0101 15:44:02.499930   61932 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 61943
I0101 15:44:02.502183   61932 main.go:141] libmachine: (minikube) DBG | Searching for 86:d7:9a:d6:bc:19 in /var/db/dhcpd_leases ...
I0101 15:44:02.502267   61932 main.go:141] libmachine: (minikube) DBG | Found 9 entries in /var/db/dhcpd_leases!
I0101 15:44:02.502278   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.10 HWAddress:ba:9a:8f:e9:3e:25 ID:1,ba:9a:8f:e9:3e:25 Lease:0x6593e37b}
I0101 15:44:02.502298   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:courageous-goose IPAddress:192.168.64.9 HWAddress:16:58:ef:f5:88:c3 ID:1,16:58:ef:f5:88:c3 Lease:0x5fe9f075}
I0101 15:44:02.502307   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:primary IPAddress:192.168.64.8 HWAddress:ca:3b:cc:e:10:69 ID:1,ca:3b:cc:e:10:69 Lease:0x5fe8b2dc}
I0101 15:44:02.502320   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:host1 IPAddress:192.168.64.7 HWAddress:1a:99:f4:ae:20:3f ID:1,1a:99:f4:ae:20:3f Lease:0x5fe7550b}
I0101 15:44:02.502332   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:node2 IPAddress:192.168.64.6 HWAddress:e6:c7:c9:68:4d:bf ID:1,e6:c7:c9:68:4d:bf Lease:0x5fe754ea}
I0101 15:44:02.502342   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:node1 IPAddress:192.168.64.5 HWAddress:42:8b:74:6d:9b:99 ID:1,42:8b:74:6d:9b:99 Lease:0x5fe754fc}
I0101 15:44:02.502395   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:control IPAddress:192.168.64.4 HWAddress:26:36:2:49:25:58 ID:1,26:36:2:49:25:58 Lease:0x5fe7550f}
I0101 15:44:02.502412   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:test1 IPAddress:192.168.64.3 HWAddress:aa:d1:73:57:5e:f3 ID:1,aa:d1:73:57:5e:f3 Lease:0x5fe6eb60}
I0101 15:44:02.502426   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:crc-ctj2r-master-0 IPAddress:192.168.64.2 HWAddress:e:f0:e3:c7:4f:be ID:1,e:f0:e3:c7:4f:be Lease:0x603213ae}
I0101 15:44:02.509603   61932 main.go:141] libmachine: (minikube) DBG | 2024/01/01 15:44:02 INFO : hyperkit: stderr: Using fd 5 for I/O notifications
I0101 15:44:02.582772   61932 main.go:141] libmachine: (minikube) DBG | 2024/01/01 15:44:02 INFO : hyperkit: stderr: /Users/ahmedabdo2070/.minikube/machines/minikube/boot2docker.iso: fcntl(F_PUNCHHOLE) Operation not permitted: block device will not support TRIM/DISCARD
I0101 15:44:02.583563   61932 main.go:141] libmachine: (minikube) DBG | 2024/01/01 15:44:02 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 2 bit: 8 unspecified don't care: bit is 0
I0101 15:44:02.583597   61932 main.go:141] libmachine: (minikube) DBG | 2024/01/01 15:44:02 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 2 bit: 22 unspecified don't care: bit is 0
I0101 15:44:02.583608   61932 main.go:141] libmachine: (minikube) DBG | 2024/01/01 15:44:02 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 2 bit: 25 unspecified don't care: bit is 0
I0101 15:44:02.583633   61932 main.go:141] libmachine: (minikube) DBG | 2024/01/01 15:44:02 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 4 bit: 12 unspecified don't care: bit is 0
I0101 15:44:02.583644   61932 main.go:141] libmachine: (minikube) DBG | 2024/01/01 15:44:02 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 4 bit: 20 unspecified don't care: bit is 0
I0101 15:44:02.583655   61932 main.go:141] libmachine: (minikube) DBG | 2024/01/01 15:44:02 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 3 bit: 13 unspecified don't care: bit is 0
I0101 15:44:03.318276   61932 main.go:141] libmachine: (minikube) DBG | 2024/01/01 15:44:03 INFO : hyperkit: stderr: rdmsr to register 0x3a on vcpu 0
I0101 15:44:03.318301   61932 main.go:141] libmachine: (minikube) DBG | 2024/01/01 15:44:03 INFO : hyperkit: stderr: rdmsr to register 0x140 on vcpu 0
I0101 15:44:03.423604   61932 main.go:141] libmachine: (minikube) DBG | 2024/01/01 15:44:03 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 2 bit: 8 unspecified don't care: bit is 0
I0101 15:44:03.423629   61932 main.go:141] libmachine: (minikube) DBG | 2024/01/01 15:44:03 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 2 bit: 22 unspecified don't care: bit is 0
I0101 15:44:03.423639   61932 main.go:141] libmachine: (minikube) DBG | 2024/01/01 15:44:03 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 2 bit: 25 unspecified don't care: bit is 0
I0101 15:44:03.423644   61932 main.go:141] libmachine: (minikube) DBG | 2024/01/01 15:44:03 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 4 bit: 12 unspecified don't care: bit is 0
I0101 15:44:03.423680   61932 main.go:141] libmachine: (minikube) DBG | 2024/01/01 15:44:03 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 4 bit: 20 unspecified don't care: bit is 0
I0101 15:44:03.423695   61932 main.go:141] libmachine: (minikube) DBG | 2024/01/01 15:44:03 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 3 bit: 13 unspecified don't care: bit is 0
I0101 15:44:03.424648   61932 main.go:141] libmachine: (minikube) DBG | 2024/01/01 15:44:03 INFO : hyperkit: stderr: rdmsr to register 0x3a on vcpu 1
I0101 15:44:03.424671   61932 main.go:141] libmachine: (minikube) DBG | 2024/01/01 15:44:03 INFO : hyperkit: stderr: rdmsr to register 0x140 on vcpu 1
I0101 15:44:04.503148   61932 main.go:141] libmachine: (minikube) DBG | Attempt 1
I0101 15:44:04.503163   61932 main.go:141] libmachine: (minikube) DBG | exe=/Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0101 15:44:04.503234   61932 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 61943
I0101 15:44:04.504892   61932 main.go:141] libmachine: (minikube) DBG | Searching for 86:d7:9a:d6:bc:19 in /var/db/dhcpd_leases ...
I0101 15:44:04.504946   61932 main.go:141] libmachine: (minikube) DBG | Found 9 entries in /var/db/dhcpd_leases!
I0101 15:44:04.504953   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.10 HWAddress:ba:9a:8f:e9:3e:25 ID:1,ba:9a:8f:e9:3e:25 Lease:0x6593e37b}
I0101 15:44:04.504962   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:courageous-goose IPAddress:192.168.64.9 HWAddress:16:58:ef:f5:88:c3 ID:1,16:58:ef:f5:88:c3 Lease:0x5fe9f075}
I0101 15:44:04.504974   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:primary IPAddress:192.168.64.8 HWAddress:ca:3b:cc:e:10:69 ID:1,ca:3b:cc:e:10:69 Lease:0x5fe8b2dc}
I0101 15:44:04.504982   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:host1 IPAddress:192.168.64.7 HWAddress:1a:99:f4:ae:20:3f ID:1,1a:99:f4:ae:20:3f Lease:0x5fe7550b}
I0101 15:44:04.504989   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:node2 IPAddress:192.168.64.6 HWAddress:e6:c7:c9:68:4d:bf ID:1,e6:c7:c9:68:4d:bf Lease:0x5fe754ea}
I0101 15:44:04.504994   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:node1 IPAddress:192.168.64.5 HWAddress:42:8b:74:6d:9b:99 ID:1,42:8b:74:6d:9b:99 Lease:0x5fe754fc}
I0101 15:44:04.504999   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:control IPAddress:192.168.64.4 HWAddress:26:36:2:49:25:58 ID:1,26:36:2:49:25:58 Lease:0x5fe7550f}
I0101 15:44:04.505017   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:test1 IPAddress:192.168.64.3 HWAddress:aa:d1:73:57:5e:f3 ID:1,aa:d1:73:57:5e:f3 Lease:0x5fe6eb60}
I0101 15:44:04.505027   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:crc-ctj2r-master-0 IPAddress:192.168.64.2 HWAddress:e:f0:e3:c7:4f:be ID:1,e:f0:e3:c7:4f:be Lease:0x603213ae}
I0101 15:44:06.505446   61932 main.go:141] libmachine: (minikube) DBG | Attempt 2
I0101 15:44:06.505461   61932 main.go:141] libmachine: (minikube) DBG | exe=/Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0101 15:44:06.505500   61932 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 61943
I0101 15:44:06.506924   61932 main.go:141] libmachine: (minikube) DBG | Searching for 86:d7:9a:d6:bc:19 in /var/db/dhcpd_leases ...
I0101 15:44:06.506953   61932 main.go:141] libmachine: (minikube) DBG | Found 9 entries in /var/db/dhcpd_leases!
I0101 15:44:06.506976   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.10 HWAddress:ba:9a:8f:e9:3e:25 ID:1,ba:9a:8f:e9:3e:25 Lease:0x6593e37b}
I0101 15:44:06.506990   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:courageous-goose IPAddress:192.168.64.9 HWAddress:16:58:ef:f5:88:c3 ID:1,16:58:ef:f5:88:c3 Lease:0x5fe9f075}
I0101 15:44:06.506998   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:primary IPAddress:192.168.64.8 HWAddress:ca:3b:cc:e:10:69 ID:1,ca:3b:cc:e:10:69 Lease:0x5fe8b2dc}
I0101 15:44:06.507005   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:host1 IPAddress:192.168.64.7 HWAddress:1a:99:f4:ae:20:3f ID:1,1a:99:f4:ae:20:3f Lease:0x5fe7550b}
I0101 15:44:06.507022   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:node2 IPAddress:192.168.64.6 HWAddress:e6:c7:c9:68:4d:bf ID:1,e6:c7:c9:68:4d:bf Lease:0x5fe754ea}
I0101 15:44:06.507042   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:node1 IPAddress:192.168.64.5 HWAddress:42:8b:74:6d:9b:99 ID:1,42:8b:74:6d:9b:99 Lease:0x5fe754fc}
I0101 15:44:06.507051   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:control IPAddress:192.168.64.4 HWAddress:26:36:2:49:25:58 ID:1,26:36:2:49:25:58 Lease:0x5fe7550f}
I0101 15:44:06.507056   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:test1 IPAddress:192.168.64.3 HWAddress:aa:d1:73:57:5e:f3 ID:1,aa:d1:73:57:5e:f3 Lease:0x5fe6eb60}
I0101 15:44:06.507065   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:crc-ctj2r-master-0 IPAddress:192.168.64.2 HWAddress:e:f0:e3:c7:4f:be ID:1,e:f0:e3:c7:4f:be Lease:0x603213ae}
I0101 15:44:08.508057   61932 main.go:141] libmachine: (minikube) DBG | Attempt 3
I0101 15:44:08.508075   61932 main.go:141] libmachine: (minikube) DBG | exe=/Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0101 15:44:08.508182   61932 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 61943
I0101 15:44:08.509835   61932 main.go:141] libmachine: (minikube) DBG | Searching for 86:d7:9a:d6:bc:19 in /var/db/dhcpd_leases ...
I0101 15:44:08.509846   61932 main.go:141] libmachine: (minikube) DBG | Found 9 entries in /var/db/dhcpd_leases!
I0101 15:44:08.509852   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.10 HWAddress:ba:9a:8f:e9:3e:25 ID:1,ba:9a:8f:e9:3e:25 Lease:0x6593e37b}
I0101 15:44:08.509859   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:courageous-goose IPAddress:192.168.64.9 HWAddress:16:58:ef:f5:88:c3 ID:1,16:58:ef:f5:88:c3 Lease:0x5fe9f075}
I0101 15:44:08.509864   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:primary IPAddress:192.168.64.8 HWAddress:ca:3b:cc:e:10:69 ID:1,ca:3b:cc:e:10:69 Lease:0x5fe8b2dc}
I0101 15:44:08.509870   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:host1 IPAddress:192.168.64.7 HWAddress:1a:99:f4:ae:20:3f ID:1,1a:99:f4:ae:20:3f Lease:0x5fe7550b}
I0101 15:44:08.509880   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:node2 IPAddress:192.168.64.6 HWAddress:e6:c7:c9:68:4d:bf ID:1,e6:c7:c9:68:4d:bf Lease:0x5fe754ea}
I0101 15:44:08.509914   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:node1 IPAddress:192.168.64.5 HWAddress:42:8b:74:6d:9b:99 ID:1,42:8b:74:6d:9b:99 Lease:0x5fe754fc}
I0101 15:44:08.509922   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:control IPAddress:192.168.64.4 HWAddress:26:36:2:49:25:58 ID:1,26:36:2:49:25:58 Lease:0x5fe7550f}
I0101 15:44:08.509933   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:test1 IPAddress:192.168.64.3 HWAddress:aa:d1:73:57:5e:f3 ID:1,aa:d1:73:57:5e:f3 Lease:0x5fe6eb60}
I0101 15:44:08.509940   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:crc-ctj2r-master-0 IPAddress:192.168.64.2 HWAddress:e:f0:e3:c7:4f:be ID:1,e:f0:e3:c7:4f:be Lease:0x603213ae}
I0101 15:44:09.189103   61932 main.go:141] libmachine: (minikube) DBG | 2024/01/01 15:44:09 INFO : hyperkit: stderr: rdmsr to register 0x64d on vcpu 0
I0101 15:44:09.189138   61932 main.go:141] libmachine: (minikube) DBG | 2024/01/01 15:44:09 INFO : hyperkit: stderr: rdmsr to register 0x64e on vcpu 0
I0101 15:44:09.189145   61932 main.go:141] libmachine: (minikube) DBG | 2024/01/01 15:44:09 INFO : hyperkit: stderr: rdmsr to register 0x34 on vcpu 0
I0101 15:44:10.510916   61932 main.go:141] libmachine: (minikube) DBG | Attempt 4
I0101 15:44:10.510940   61932 main.go:141] libmachine: (minikube) DBG | exe=/Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0101 15:44:10.511061   61932 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 61943
I0101 15:44:10.512469   61932 main.go:141] libmachine: (minikube) DBG | Searching for 86:d7:9a:d6:bc:19 in /var/db/dhcpd_leases ...
I0101 15:44:10.512510   61932 main.go:141] libmachine: (minikube) DBG | Found 9 entries in /var/db/dhcpd_leases!
I0101 15:44:10.512531   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.10 HWAddress:ba:9a:8f:e9:3e:25 ID:1,ba:9a:8f:e9:3e:25 Lease:0x6593e37b}
I0101 15:44:10.512545   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:courageous-goose IPAddress:192.168.64.9 HWAddress:16:58:ef:f5:88:c3 ID:1,16:58:ef:f5:88:c3 Lease:0x5fe9f075}
I0101 15:44:10.512553   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:primary IPAddress:192.168.64.8 HWAddress:ca:3b:cc:e:10:69 ID:1,ca:3b:cc:e:10:69 Lease:0x5fe8b2dc}
I0101 15:44:10.512558   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:host1 IPAddress:192.168.64.7 HWAddress:1a:99:f4:ae:20:3f ID:1,1a:99:f4:ae:20:3f Lease:0x5fe7550b}
I0101 15:44:10.512571   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:node2 IPAddress:192.168.64.6 HWAddress:e6:c7:c9:68:4d:bf ID:1,e6:c7:c9:68:4d:bf Lease:0x5fe754ea}
I0101 15:44:10.512578   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:node1 IPAddress:192.168.64.5 HWAddress:42:8b:74:6d:9b:99 ID:1,42:8b:74:6d:9b:99 Lease:0x5fe754fc}
I0101 15:44:10.512583   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:control IPAddress:192.168.64.4 HWAddress:26:36:2:49:25:58 ID:1,26:36:2:49:25:58 Lease:0x5fe7550f}
I0101 15:44:10.512588   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:test1 IPAddress:192.168.64.3 HWAddress:aa:d1:73:57:5e:f3 ID:1,aa:d1:73:57:5e:f3 Lease:0x5fe6eb60}
I0101 15:44:10.512604   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:crc-ctj2r-master-0 IPAddress:192.168.64.2 HWAddress:e:f0:e3:c7:4f:be ID:1,e:f0:e3:c7:4f:be Lease:0x603213ae}
I0101 15:44:12.513252   61932 main.go:141] libmachine: (minikube) DBG | Attempt 5
I0101 15:44:12.513262   61932 main.go:141] libmachine: (minikube) DBG | exe=/Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0101 15:44:12.513367   61932 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 61943
I0101 15:44:12.514762   61932 main.go:141] libmachine: (minikube) DBG | Searching for 86:d7:9a:d6:bc:19 in /var/db/dhcpd_leases ...
I0101 15:44:12.514801   61932 main.go:141] libmachine: (minikube) DBG | Found 9 entries in /var/db/dhcpd_leases!
I0101 15:44:12.514807   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.10 HWAddress:ba:9a:8f:e9:3e:25 ID:1,ba:9a:8f:e9:3e:25 Lease:0x6593e37b}
I0101 15:44:12.514815   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:courageous-goose IPAddress:192.168.64.9 HWAddress:16:58:ef:f5:88:c3 ID:1,16:58:ef:f5:88:c3 Lease:0x5fe9f075}
I0101 15:44:12.514823   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:primary IPAddress:192.168.64.8 HWAddress:ca:3b:cc:e:10:69 ID:1,ca:3b:cc:e:10:69 Lease:0x5fe8b2dc}
I0101 15:44:12.514831   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:host1 IPAddress:192.168.64.7 HWAddress:1a:99:f4:ae:20:3f ID:1,1a:99:f4:ae:20:3f Lease:0x5fe7550b}
I0101 15:44:12.514836   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:node2 IPAddress:192.168.64.6 HWAddress:e6:c7:c9:68:4d:bf ID:1,e6:c7:c9:68:4d:bf Lease:0x5fe754ea}
I0101 15:44:12.514854   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:node1 IPAddress:192.168.64.5 HWAddress:42:8b:74:6d:9b:99 ID:1,42:8b:74:6d:9b:99 Lease:0x5fe754fc}
I0101 15:44:12.514862   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:control IPAddress:192.168.64.4 HWAddress:26:36:2:49:25:58 ID:1,26:36:2:49:25:58 Lease:0x5fe7550f}
I0101 15:44:12.514882   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:test1 IPAddress:192.168.64.3 HWAddress:aa:d1:73:57:5e:f3 ID:1,aa:d1:73:57:5e:f3 Lease:0x5fe6eb60}
I0101 15:44:12.514892   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:crc-ctj2r-master-0 IPAddress:192.168.64.2 HWAddress:e:f0:e3:c7:4f:be ID:1,e:f0:e3:c7:4f:be Lease:0x603213ae}
I0101 15:44:14.515398   61932 main.go:141] libmachine: (minikube) DBG | Attempt 6
I0101 15:44:14.515435   61932 main.go:141] libmachine: (minikube) DBG | exe=/Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0101 15:44:14.515707   61932 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 61943
I0101 15:44:14.520346   61932 main.go:141] libmachine: (minikube) DBG | Searching for 86:d7:9a:d6:bc:19 in /var/db/dhcpd_leases ...
I0101 15:44:14.520394   61932 main.go:141] libmachine: (minikube) DBG | Found 10 entries in /var/db/dhcpd_leases!
I0101 15:44:14.520411   61932 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.11 HWAddress:86:d7:9a:d6:bc:19 ID:1,86:d7:9a:d6:bc:19 Lease:0x6593f70d}
I0101 15:44:14.520425   61932 main.go:141] libmachine: (minikube) DBG | Found match: 86:d7:9a:d6:bc:19
I0101 15:44:14.520435   61932 main.go:141] libmachine: (minikube) DBG | IP: 192.168.64.11
I0101 15:44:14.520583   61932 main.go:141] libmachine: (minikube) Calling .GetConfigRaw
I0101 15:44:14.522183   61932 main.go:141] libmachine: (minikube) Calling .DriverName
I0101 15:44:14.522555   61932 main.go:141] libmachine: (minikube) Calling .DriverName
I0101 15:44:14.522914   61932 main.go:141] libmachine: Waiting for machine to be running, this may take a few minutes...
I0101 15:44:14.522933   61932 main.go:141] libmachine: (minikube) Calling .GetState
I0101 15:44:14.523197   61932 main.go:141] libmachine: (minikube) DBG | exe=/Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0101 15:44:14.523329   61932 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 61943
I0101 15:44:14.526702   61932 main.go:141] libmachine: Detecting operating system of created instance...
I0101 15:44:14.526720   61932 main.go:141] libmachine: Waiting for SSH to be available...
I0101 15:44:14.526728   61932 main.go:141] libmachine: Getting to WaitForSSH function...
I0101 15:44:14.526741   61932 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0101 15:44:14.526991   61932 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0101 15:44:14.527177   61932 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0101 15:44:14.527352   61932 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0101 15:44:14.527545   61932 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0101 15:44:14.528345   61932 main.go:141] libmachine: Using SSH client type: native
I0101 15:44:14.529107   61932 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10040a380] 0x10040d060 <nil>  [] 0s} 192.168.64.11 22 <nil> <nil>}
I0101 15:44:14.529117   61932 main.go:141] libmachine: About to run SSH command:
exit 0
I0101 15:44:14.602934   61932 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0101 15:44:14.602959   61932 main.go:141] libmachine: Detecting the provisioner...
I0101 15:44:14.602965   61932 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0101 15:44:14.603182   61932 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0101 15:44:14.603306   61932 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0101 15:44:14.603432   61932 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0101 15:44:14.603561   61932 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0101 15:44:14.603770   61932 main.go:141] libmachine: Using SSH client type: native
I0101 15:44:14.604150   61932 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10040a380] 0x10040d060 <nil>  [] 0s} 192.168.64.11 22 <nil> <nil>}
I0101 15:44:14.604159   61932 main.go:141] libmachine: About to run SSH command:
cat /etc/os-release
I0101 15:44:14.672490   61932 main.go:141] libmachine: SSH cmd err, output: <nil>: NAME=Buildroot
VERSION=2021.02.12-1-gb75713b-dirty
ID=buildroot
VERSION_ID=2021.02.12
PRETTY_NAME="Buildroot 2021.02.12"

I0101 15:44:14.672546   61932 main.go:141] libmachine: found compatible host: buildroot
I0101 15:44:14.672552   61932 main.go:141] libmachine: Provisioning with buildroot...
I0101 15:44:14.672559   61932 main.go:141] libmachine: (minikube) Calling .GetMachineName
I0101 15:44:14.672794   61932 buildroot.go:166] provisioning hostname "minikube"
I0101 15:44:14.672807   61932 main.go:141] libmachine: (minikube) Calling .GetMachineName
I0101 15:44:14.672974   61932 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0101 15:44:14.673107   61932 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0101 15:44:14.673241   61932 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0101 15:44:14.673417   61932 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0101 15:44:14.673588   61932 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0101 15:44:14.673824   61932 main.go:141] libmachine: Using SSH client type: native
I0101 15:44:14.674256   61932 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10040a380] 0x10040d060 <nil>  [] 0s} 192.168.64.11 22 <nil> <nil>}
I0101 15:44:14.674267   61932 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0101 15:44:14.758813   61932 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0101 15:44:14.758830   61932 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0101 15:44:14.759081   61932 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0101 15:44:14.759203   61932 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0101 15:44:14.759321   61932 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0101 15:44:14.759431   61932 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0101 15:44:14.759669   61932 main.go:141] libmachine: Using SSH client type: native
I0101 15:44:14.760046   61932 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10040a380] 0x10040d060 <nil>  [] 0s} 192.168.64.11 22 <nil> <nil>}
I0101 15:44:14.760056   61932 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0101 15:44:14.834426   61932 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0101 15:44:14.834453   61932 buildroot.go:172] set auth options {CertDir:/Users/ahmedabdo2070/.minikube CaCertPath:/Users/ahmedabdo2070/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/ahmedabdo2070/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/ahmedabdo2070/.minikube/machines/server.pem ServerKeyPath:/Users/ahmedabdo2070/.minikube/machines/server-key.pem ClientKeyPath:/Users/ahmedabdo2070/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/ahmedabdo2070/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/ahmedabdo2070/.minikube}
I0101 15:44:14.834466   61932 buildroot.go:174] setting up certificates
I0101 15:44:14.834478   61932 provision.go:83] configureAuth start
I0101 15:44:14.834484   61932 main.go:141] libmachine: (minikube) Calling .GetMachineName
I0101 15:44:14.834689   61932 main.go:141] libmachine: (minikube) Calling .GetIP
I0101 15:44:14.834831   61932 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0101 15:44:14.834983   61932 provision.go:138] copyHostCerts
I0101 15:44:14.835133   61932 exec_runner.go:144] found /Users/ahmedabdo2070/.minikube/key.pem, removing ...
I0101 15:44:14.835142   61932 exec_runner.go:203] rm: /Users/ahmedabdo2070/.minikube/key.pem
I0101 15:44:14.835321   61932 exec_runner.go:151] cp: /Users/ahmedabdo2070/.minikube/certs/key.pem --> /Users/ahmedabdo2070/.minikube/key.pem (1675 bytes)
I0101 15:44:14.836496   61932 exec_runner.go:144] found /Users/ahmedabdo2070/.minikube/ca.pem, removing ...
I0101 15:44:14.836501   61932 exec_runner.go:203] rm: /Users/ahmedabdo2070/.minikube/ca.pem
I0101 15:44:14.836628   61932 exec_runner.go:151] cp: /Users/ahmedabdo2070/.minikube/certs/ca.pem --> /Users/ahmedabdo2070/.minikube/ca.pem (1099 bytes)
I0101 15:44:14.836861   61932 exec_runner.go:144] found /Users/ahmedabdo2070/.minikube/cert.pem, removing ...
I0101 15:44:14.836864   61932 exec_runner.go:203] rm: /Users/ahmedabdo2070/.minikube/cert.pem
I0101 15:44:14.836970   61932 exec_runner.go:151] cp: /Users/ahmedabdo2070/.minikube/certs/cert.pem --> /Users/ahmedabdo2070/.minikube/cert.pem (1139 bytes)
I0101 15:44:14.837124   61932 provision.go:112] generating server cert: /Users/ahmedabdo2070/.minikube/machines/server.pem ca-key=/Users/ahmedabdo2070/.minikube/certs/ca.pem private-key=/Users/ahmedabdo2070/.minikube/certs/ca-key.pem org=ahmedabdo2070.minikube san=[192.168.64.11 192.168.64.11 localhost 127.0.0.1 minikube minikube]
I0101 15:44:14.981094   61932 provision.go:172] copyRemoteCerts
I0101 15:44:14.981195   61932 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0101 15:44:14.981249   61932 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0101 15:44:14.981565   61932 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0101 15:44:14.981737   61932 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0101 15:44:14.981902   61932 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0101 15:44:14.981985   61932 sshutil.go:53] new ssh client: &{IP:192.168.64.11 Port:22 SSHKeyPath:/Users/ahmedabdo2070/.minikube/machines/minikube/id_rsa Username:docker}
I0101 15:44:15.026114   61932 ssh_runner.go:362] scp /Users/ahmedabdo2070/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1099 bytes)
I0101 15:44:15.050403   61932 ssh_runner.go:362] scp /Users/ahmedabdo2070/.minikube/machines/server.pem --> /etc/docker/server.pem (1220 bytes)
I0101 15:44:15.071533   61932 ssh_runner.go:362] scp /Users/ahmedabdo2070/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0101 15:44:15.093000   61932 provision.go:86] duration metric: configureAuth took 258.506651ms
I0101 15:44:15.093014   61932 buildroot.go:189] setting minikube options for container-runtime
I0101 15:44:15.093204   61932 config.go:182] Loaded profile config "minikube": Driver=hyperkit, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0101 15:44:15.093215   61932 main.go:141] libmachine: (minikube) Calling .DriverName
I0101 15:44:15.093426   61932 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0101 15:44:15.093619   61932 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0101 15:44:15.093716   61932 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0101 15:44:15.093824   61932 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0101 15:44:15.094050   61932 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0101 15:44:15.094245   61932 main.go:141] libmachine: Using SSH client type: native
I0101 15:44:15.094589   61932 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10040a380] 0x10040d060 <nil>  [] 0s} 192.168.64.11 22 <nil> <nil>}
I0101 15:44:15.094598   61932 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0101 15:44:15.161754   61932 main.go:141] libmachine: SSH cmd err, output: <nil>: tmpfs

I0101 15:44:15.161765   61932 buildroot.go:70] root file system type: tmpfs
I0101 15:44:15.161887   61932 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0101 15:44:15.161905   61932 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0101 15:44:15.162161   61932 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0101 15:44:15.162359   61932 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0101 15:44:15.162564   61932 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0101 15:44:15.162775   61932 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0101 15:44:15.163013   61932 main.go:141] libmachine: Using SSH client type: native
I0101 15:44:15.163503   61932 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10040a380] 0x10040d060 <nil>  [] 0s} 192.168.64.11 22 <nil> <nil>}
I0101 15:44:15.163598   61932 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=hyperkit --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0101 15:44:15.242278   61932 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=hyperkit --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0101 15:44:15.242301   61932 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0101 15:44:15.242528   61932 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0101 15:44:15.242674   61932 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0101 15:44:15.242813   61932 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0101 15:44:15.242954   61932 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0101 15:44:15.243154   61932 main.go:141] libmachine: Using SSH client type: native
I0101 15:44:15.243588   61932 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10040a380] 0x10040d060 <nil>  [] 0s} 192.168.64.11 22 <nil> <nil>}
I0101 15:44:15.243606   61932 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0101 15:44:15.797839   61932 main.go:141] libmachine: SSH cmd err, output: <nil>: diff: can't stat '/lib/systemd/system/docker.service': No such file or directory
Created symlink /etc/systemd/system/multi-user.target.wants/docker.service ‚Üí /usr/lib/systemd/system/docker.service.

I0101 15:44:15.797851   61932 main.go:141] libmachine: Checking connection to Docker...
I0101 15:44:15.797858   61932 main.go:141] libmachine: (minikube) Calling .GetURL
I0101 15:44:15.798027   61932 main.go:141] libmachine: Docker is up and running!
I0101 15:44:15.798032   61932 main.go:141] libmachine: Reticulating splines...
I0101 15:44:15.798036   61932 client.go:171] LocalClient.Create took 14.295477154s
I0101 15:44:15.798052   61932 start.go:167] duration metric: libmachine.API.Create for "minikube" took 14.295551814s
I0101 15:44:15.798062   61932 start.go:300] post-start starting for "minikube" (driver="hyperkit")
I0101 15:44:15.798071   61932 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0101 15:44:15.798080   61932 main.go:141] libmachine: (minikube) Calling .DriverName
I0101 15:44:15.798286   61932 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0101 15:44:15.798296   61932 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0101 15:44:15.798391   61932 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0101 15:44:15.798481   61932 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0101 15:44:15.798575   61932 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0101 15:44:15.798690   61932 sshutil.go:53] new ssh client: &{IP:192.168.64.11 Port:22 SSHKeyPath:/Users/ahmedabdo2070/.minikube/machines/minikube/id_rsa Username:docker}
I0101 15:44:15.837977   61932 ssh_runner.go:195] Run: cat /etc/os-release
I0101 15:44:15.841745   61932 info.go:137] Remote host: Buildroot 2021.02.12
I0101 15:44:15.841759   61932 filesync.go:126] Scanning /Users/ahmedabdo2070/.minikube/addons for local assets ...
I0101 15:44:15.843145   61932 filesync.go:126] Scanning /Users/ahmedabdo2070/.minikube/files for local assets ...
I0101 15:44:15.843213   61932 start.go:303] post-start completed in 45.146311ms
I0101 15:44:15.843236   61932 main.go:141] libmachine: (minikube) Calling .GetConfigRaw
I0101 15:44:15.843917   61932 main.go:141] libmachine: (minikube) Calling .GetIP
I0101 15:44:15.844083   61932 profile.go:148] Saving config to /Users/ahmedabdo2070/.minikube/profiles/minikube/config.json ...
I0101 15:44:15.844428   61932 start.go:128] duration metric: createHost completed in 14.38588016s
I0101 15:44:15.844445   61932 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0101 15:44:15.844542   61932 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0101 15:44:15.844637   61932 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0101 15:44:15.844729   61932 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0101 15:44:15.844838   61932 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0101 15:44:15.844985   61932 main.go:141] libmachine: Using SSH client type: native
I0101 15:44:15.845284   61932 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10040a380] 0x10040d060 <nil>  [] 0s} 192.168.64.11 22 <nil> <nil>}
I0101 15:44:15.845289   61932 main.go:141] libmachine: About to run SSH command:
date +%!s(MISSING).%!N(MISSING)
I0101 15:44:15.910274   61932 main.go:141] libmachine: SSH cmd err, output: <nil>: 1704109455.594084876

I0101 15:44:15.910284   61932 fix.go:206] guest clock: 1704109455.594084876
I0101 15:44:15.910289   61932 fix.go:219] Guest: 2024-01-01 15:44:15.594084876 +0400 +04 Remote: 2024-01-01 15:44:15.844437 +0400 +04 m=+14.680663561 (delta=-250.352124ms)
I0101 15:44:15.910309   61932 fix.go:190] guest clock delta is within tolerance: -250.352124ms
I0101 15:44:15.910312   61932 start.go:83] releasing machines lock for "minikube", held for 14.45198436s
I0101 15:44:15.910332   61932 main.go:141] libmachine: (minikube) Calling .DriverName
I0101 15:44:15.910498   61932 main.go:141] libmachine: (minikube) Calling .GetIP
I0101 15:44:15.910608   61932 main.go:141] libmachine: (minikube) Calling .DriverName
I0101 15:44:15.911118   61932 main.go:141] libmachine: (minikube) Calling .DriverName
I0101 15:44:15.911291   61932 main.go:141] libmachine: (minikube) Calling .DriverName
I0101 15:44:15.911505   61932 ssh_runner.go:195] Run: cat /version.json
I0101 15:44:15.911517   61932 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0101 15:44:15.911629   61932 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0101 15:44:15.911738   61932 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0101 15:44:15.911856   61932 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0101 15:44:15.911946   61932 sshutil.go:53] new ssh client: &{IP:192.168.64.11 Port:22 SSHKeyPath:/Users/ahmedabdo2070/.minikube/machines/minikube/id_rsa Username:docker}
I0101 15:44:15.912521   61932 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0101 15:44:15.912555   61932 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0101 15:44:15.912714   61932 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0101 15:44:15.912816   61932 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0101 15:44:15.912921   61932 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0101 15:44:15.913003   61932 sshutil.go:53] new ssh client: &{IP:192.168.64.11 Port:22 SSHKeyPath:/Users/ahmedabdo2070/.minikube/machines/minikube/id_rsa Username:docker}
I0101 15:44:15.954034   61932 ssh_runner.go:195] Run: systemctl --version
I0101 15:44:15.966401   61932 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
W0101 15:44:16.612301   61932 cni.go:209] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I0101 15:44:16.612571   61932 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0101 15:44:16.638723   61932 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0101 15:44:16.638736   61932 start.go:472] detecting cgroup driver to use...
I0101 15:44:16.638895   61932 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0101 15:44:16.656472   61932 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0101 15:44:16.665406   61932 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0101 15:44:16.673370   61932 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0101 15:44:16.673461   61932 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0101 15:44:16.681661   61932 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0101 15:44:16.689852   61932 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0101 15:44:16.698489   61932 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0101 15:44:16.707188   61932 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0101 15:44:16.715807   61932 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0101 15:44:16.724277   61932 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0101 15:44:16.731706   61932 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0101 15:44:16.739292   61932 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0101 15:44:16.827008   61932 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0101 15:44:16.843223   61932 start.go:472] detecting cgroup driver to use...
I0101 15:44:16.843337   61932 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0101 15:44:16.855865   61932 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0101 15:44:16.870234   61932 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I0101 15:44:16.892216   61932 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0101 15:44:16.903082   61932 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0101 15:44:16.915206   61932 ssh_runner.go:195] Run: sudo systemctl stop -f crio
I0101 15:44:16.938097   61932 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0101 15:44:16.949929   61932 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0101 15:44:16.965885   61932 ssh_runner.go:195] Run: which cri-dockerd
I0101 15:44:16.969005   61932 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0101 15:44:16.976186   61932 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0101 15:44:16.988942   61932 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0101 15:44:17.081870   61932 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0101 15:44:17.181992   61932 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I0101 15:44:17.182077   61932 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0101 15:44:17.197385   61932 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0101 15:44:17.296439   61932 ssh_runner.go:195] Run: sudo systemctl restart docker
I0101 15:44:18.610081   61932 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.313621145s)
I0101 15:44:18.610175   61932 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0101 15:44:18.699637   61932 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0101 15:44:18.794771   61932 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0101 15:44:18.893048   61932 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0101 15:44:18.991863   61932 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0101 15:44:19.008974   61932 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0101 15:44:19.108702   61932 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0101 15:44:19.175085   61932 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0101 15:44:19.176388   61932 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0101 15:44:19.180698   61932 start.go:540] Will wait 60s for crictl version
I0101 15:44:19.180791   61932 ssh_runner.go:195] Run: which crictl
I0101 15:44:19.183939   61932 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0101 15:44:19.225022   61932 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0101 15:44:19.225099   61932 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0101 15:44:19.244641   61932 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0101 15:44:19.284998   61932 out.go:204] üê≥  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0101 15:44:19.285155   61932 main.go:141] libmachine: (minikube) Calling .GetIP
I0101 15:44:19.286876   61932 ssh_runner.go:195] Run: grep 192.168.64.1	host.minikube.internal$ /etc/hosts
I0101 15:44:19.294563   61932 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.64.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0101 15:44:19.313684   61932 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0101 15:44:19.313773   61932 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0101 15:44:19.329547   61932 docker.go:671] Got preloaded images: 
I0101 15:44:19.329555   61932 docker.go:677] registry.k8s.io/kube-apiserver:v1.28.3 wasn't preloaded
I0101 15:44:19.329659   61932 ssh_runner.go:195] Run: sudo cat /var/lib/docker/image/overlay2/repositories.json
I0101 15:44:19.336685   61932 ssh_runner.go:195] Run: which lz4
I0101 15:44:19.340310   61932 ssh_runner.go:195] Run: stat -c "%!s(MISSING) %!y(MISSING)" /preloaded.tar.lz4
I0101 15:44:19.343214   61932 ssh_runner.go:352] existence check for /preloaded.tar.lz4: stat -c "%!s(MISSING) %!y(MISSING)" /preloaded.tar.lz4: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/preloaded.tar.lz4': No such file or directory
I0101 15:44:19.343239   61932 ssh_runner.go:362] scp /Users/ahmedabdo2070/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 --> /preloaded.tar.lz4 (422944352 bytes)
I0101 15:44:20.760527   61932 docker.go:635] Took 1.420279 seconds to copy over tarball
I0101 15:44:20.760642   61932 ssh_runner.go:195] Run: sudo tar -I lz4 -C /var -xf /preloaded.tar.lz4
I0101 15:44:25.405720   61932 ssh_runner.go:235] Completed: sudo tar -I lz4 -C /var -xf /preloaded.tar.lz4: (4.645027005s)
I0101 15:44:25.405732   61932 ssh_runner.go:146] rm: /preloaded.tar.lz4
I0101 15:44:25.437533   61932 ssh_runner.go:195] Run: sudo cat /var/lib/docker/image/overlay2/repositories.json
I0101 15:44:25.448092   61932 ssh_runner.go:362] scp memory --> /var/lib/docker/image/overlay2/repositories.json (2629 bytes)
I0101 15:44:25.462806   61932 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0101 15:44:25.553015   61932 ssh_runner.go:195] Run: sudo systemctl restart docker
I0101 15:44:27.741446   61932 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.188400662s)
I0101 15:44:27.741546   61932 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0101 15:44:27.756865   61932 docker.go:671] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0101 15:44:27.756885   61932 cache_images.go:84] Images are preloaded, skipping loading
I0101 15:44:27.756977   61932 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0101 15:44:27.778208   61932 cni.go:84] Creating CNI manager for ""
I0101 15:44:27.778216   61932 cni.go:136] 1 nodes found, recommending kindnet
I0101 15:44:27.778231   61932 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0101 15:44:27.778247   61932 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.64.11 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.64.11"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.64.11 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0101 15:44:27.778386   61932 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.64.11
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.64.11
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.64.11"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0101 15:44:27.778451   61932 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.64.11

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0101 15:44:27.778554   61932 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0101 15:44:27.785568   61932 binaries.go:44] Found k8s binaries, skipping transfer
I0101 15:44:27.785663   61932 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0101 15:44:27.792644   61932 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (370 bytes)
I0101 15:44:27.806397   61932 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0101 15:44:27.819799   61932 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2094 bytes)
I0101 15:44:27.832374   61932 ssh_runner.go:195] Run: grep 192.168.64.11	control-plane.minikube.internal$ /etc/hosts
I0101 15:44:27.835397   61932 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.64.11	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0101 15:44:27.844955   61932 certs.go:56] Setting up /Users/ahmedabdo2070/.minikube/profiles/minikube for IP: 192.168.64.11
I0101 15:44:27.844970   61932 certs.go:190] acquiring lock for shared ca certs: {Name:mk38710522a6e902cc2f3c0842e5eb6338f09966 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0101 15:44:27.845511   61932 certs.go:199] skipping minikubeCA CA generation: /Users/ahmedabdo2070/.minikube/ca.key
I0101 15:44:27.847574   61932 certs.go:199] skipping proxyClientCA CA generation: /Users/ahmedabdo2070/.minikube/proxy-client-ca.key
I0101 15:44:27.847638   61932 certs.go:319] generating minikube-user signed cert: /Users/ahmedabdo2070/.minikube/profiles/minikube/client.key
I0101 15:44:27.847649   61932 crypto.go:68] Generating cert /Users/ahmedabdo2070/.minikube/profiles/minikube/client.crt with IP's: []
I0101 15:44:27.994957   61932 crypto.go:156] Writing cert to /Users/ahmedabdo2070/.minikube/profiles/minikube/client.crt ...
I0101 15:44:27.994968   61932 lock.go:35] WriteFile acquiring /Users/ahmedabdo2070/.minikube/profiles/minikube/client.crt: {Name:mk61b4f08b32e0c86958addd14e059cab3413a1c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0101 15:44:27.995488   61932 crypto.go:164] Writing key to /Users/ahmedabdo2070/.minikube/profiles/minikube/client.key ...
I0101 15:44:27.995495   61932 lock.go:35] WriteFile acquiring /Users/ahmedabdo2070/.minikube/profiles/minikube/client.key: {Name:mkfbc2a2682e118c30a6c3b2e0950f1a02e6059e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0101 15:44:27.995856   61932 certs.go:319] generating minikube signed cert: /Users/ahmedabdo2070/.minikube/profiles/minikube/apiserver.key.52076033
I0101 15:44:27.995865   61932 crypto.go:68] Generating cert /Users/ahmedabdo2070/.minikube/profiles/minikube/apiserver.crt.52076033 with IP's: [192.168.64.11 10.96.0.1 127.0.0.1 10.0.0.1]
I0101 15:44:28.077555   61932 crypto.go:156] Writing cert to /Users/ahmedabdo2070/.minikube/profiles/minikube/apiserver.crt.52076033 ...
I0101 15:44:28.077566   61932 lock.go:35] WriteFile acquiring /Users/ahmedabdo2070/.minikube/profiles/minikube/apiserver.crt.52076033: {Name:mka777f377e32b7d6cb33a1165ee31c14a8c1374 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0101 15:44:28.077911   61932 crypto.go:164] Writing key to /Users/ahmedabdo2070/.minikube/profiles/minikube/apiserver.key.52076033 ...
I0101 15:44:28.077916   61932 lock.go:35] WriteFile acquiring /Users/ahmedabdo2070/.minikube/profiles/minikube/apiserver.key.52076033: {Name:mk70e82b147b03f2ad1b74d1c608bb4cbd25660d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0101 15:44:28.078171   61932 certs.go:337] copying /Users/ahmedabdo2070/.minikube/profiles/minikube/apiserver.crt.52076033 -> /Users/ahmedabdo2070/.minikube/profiles/minikube/apiserver.crt
I0101 15:44:28.078327   61932 certs.go:341] copying /Users/ahmedabdo2070/.minikube/profiles/minikube/apiserver.key.52076033 -> /Users/ahmedabdo2070/.minikube/profiles/minikube/apiserver.key
I0101 15:44:28.078498   61932 certs.go:319] generating aggregator signed cert: /Users/ahmedabdo2070/.minikube/profiles/minikube/proxy-client.key
I0101 15:44:28.078508   61932 crypto.go:68] Generating cert /Users/ahmedabdo2070/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0101 15:44:28.197765   61932 crypto.go:156] Writing cert to /Users/ahmedabdo2070/.minikube/profiles/minikube/proxy-client.crt ...
I0101 15:44:28.197775   61932 lock.go:35] WriteFile acquiring /Users/ahmedabdo2070/.minikube/profiles/minikube/proxy-client.crt: {Name:mk2bd67bf2dcea31add4cc33490c6d5dc60d55fe Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0101 15:44:28.198223   61932 crypto.go:164] Writing key to /Users/ahmedabdo2070/.minikube/profiles/minikube/proxy-client.key ...
I0101 15:44:28.198229   61932 lock.go:35] WriteFile acquiring /Users/ahmedabdo2070/.minikube/profiles/minikube/proxy-client.key: {Name:mka7f4d080a65146bd21aaaf7208d7d463d73278 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0101 15:44:28.199576   61932 certs.go:437] found cert: /Users/ahmedabdo2070/.minikube/certs/Users/ahmedabdo2070/.minikube/certs/ca-key.pem (1675 bytes)
I0101 15:44:28.199635   61932 certs.go:437] found cert: /Users/ahmedabdo2070/.minikube/certs/Users/ahmedabdo2070/.minikube/certs/ca.pem (1099 bytes)
I0101 15:44:28.199664   61932 certs.go:437] found cert: /Users/ahmedabdo2070/.minikube/certs/Users/ahmedabdo2070/.minikube/certs/cert.pem (1139 bytes)
I0101 15:44:28.199699   61932 certs.go:437] found cert: /Users/ahmedabdo2070/.minikube/certs/Users/ahmedabdo2070/.minikube/certs/key.pem (1675 bytes)
I0101 15:44:28.200241   61932 ssh_runner.go:362] scp /Users/ahmedabdo2070/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0101 15:44:28.223456   61932 ssh_runner.go:362] scp /Users/ahmedabdo2070/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0101 15:44:28.244146   61932 ssh_runner.go:362] scp /Users/ahmedabdo2070/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0101 15:44:28.262108   61932 ssh_runner.go:362] scp /Users/ahmedabdo2070/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0101 15:44:28.283380   61932 ssh_runner.go:362] scp /Users/ahmedabdo2070/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0101 15:44:28.303186   61932 ssh_runner.go:362] scp /Users/ahmedabdo2070/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0101 15:44:28.321935   61932 ssh_runner.go:362] scp /Users/ahmedabdo2070/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0101 15:44:28.340212   61932 ssh_runner.go:362] scp /Users/ahmedabdo2070/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0101 15:44:28.358763   61932 ssh_runner.go:362] scp /Users/ahmedabdo2070/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0101 15:44:28.378603   61932 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0101 15:44:28.392073   61932 ssh_runner.go:195] Run: openssl version
I0101 15:44:28.396472   61932 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0101 15:44:28.405962   61932 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0101 15:44:28.409545   61932 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Oct 22  2020 /usr/share/ca-certificates/minikubeCA.pem
I0101 15:44:28.409603   61932 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0101 15:44:28.413862   61932 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0101 15:44:28.421585   61932 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0101 15:44:28.424793   61932 certs.go:353] certs directory doesn't exist, likely first start: ls /var/lib/minikube/certs/etcd: Process exited with status 2
stdout:

stderr:
ls: cannot access '/var/lib/minikube/certs/etcd': No such file or directory
I0101 15:44:28.424838   61932 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.32.1-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:8192 CPUs:2 DiskSize:20000 VMDriver: Driver:hyperkit HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.64.11 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0101 15:44:28.424929   61932 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0101 15:44:28.438947   61932 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0101 15:44:28.445952   61932 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0101 15:44:28.453032   61932 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0101 15:44:28.459898   61932 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0101 15:44:28.459923   61932 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem"
I0101 15:44:28.503080   61932 kubeadm.go:322] [init] Using Kubernetes version: v1.28.3
I0101 15:44:28.503137   61932 kubeadm.go:322] [preflight] Running pre-flight checks
I0101 15:44:28.627332   61932 kubeadm.go:322] [preflight] Pulling images required for setting up a Kubernetes cluster
I0101 15:44:28.627435   61932 kubeadm.go:322] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0101 15:44:28.627518   61932 kubeadm.go:322] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0101 15:44:28.872830   61932 kubeadm.go:322] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0101 15:44:28.930229   61932 out.go:204]     ‚ñ™ Generating certificates and keys ...
I0101 15:44:28.930338   61932 kubeadm.go:322] [certs] Using existing ca certificate authority
I0101 15:44:28.930402   61932 kubeadm.go:322] [certs] Using existing apiserver certificate and key on disk
I0101 15:44:29.145249   61932 kubeadm.go:322] [certs] Generating "apiserver-kubelet-client" certificate and key
I0101 15:44:29.331524   61932 kubeadm.go:322] [certs] Generating "front-proxy-ca" certificate and key
I0101 15:44:29.632555   61932 kubeadm.go:322] [certs] Generating "front-proxy-client" certificate and key
I0101 15:44:29.730288   61932 kubeadm.go:322] [certs] Generating "etcd/ca" certificate and key
I0101 15:44:30.091588   61932 kubeadm.go:322] [certs] Generating "etcd/server" certificate and key
I0101 15:44:30.091871   61932 kubeadm.go:322] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.64.11 127.0.0.1 ::1]
I0101 15:44:30.430651   61932 kubeadm.go:322] [certs] Generating "etcd/peer" certificate and key
I0101 15:44:30.430943   61932 kubeadm.go:322] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.64.11 127.0.0.1 ::1]
I0101 15:44:30.515591   61932 kubeadm.go:322] [certs] Generating "etcd/healthcheck-client" certificate and key
I0101 15:44:30.707888   61932 kubeadm.go:322] [certs] Generating "apiserver-etcd-client" certificate and key
I0101 15:44:30.947215   61932 kubeadm.go:322] [certs] Generating "sa" key and public key
I0101 15:44:30.947622   61932 kubeadm.go:322] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0101 15:44:31.126768   61932 kubeadm.go:322] [kubeconfig] Writing "admin.conf" kubeconfig file
I0101 15:44:31.197458   61932 kubeadm.go:322] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0101 15:44:31.456768   61932 kubeadm.go:322] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0101 15:44:31.567564   61932 kubeadm.go:322] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0101 15:44:31.568190   61932 kubeadm.go:322] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0101 15:44:31.572206   61932 kubeadm.go:322] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0101 15:44:31.592295   61932 out.go:204]     ‚ñ™ Booting up control plane ...
I0101 15:44:31.592393   61932 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0101 15:44:31.592496   61932 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0101 15:44:31.592627   61932 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0101 15:44:31.592771   61932 kubeadm.go:322] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0101 15:44:31.592877   61932 kubeadm.go:322] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0101 15:44:31.592935   61932 kubeadm.go:322] [kubelet-start] Starting the kubelet
I0101 15:44:31.688729   61932 kubeadm.go:322] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
I0101 15:44:37.690400   61932 kubeadm.go:322] [apiclient] All control plane components are healthy after 6.005718 seconds
I0101 15:44:37.690898   61932 kubeadm.go:322] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0101 15:44:37.715039   61932 kubeadm.go:322] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0101 15:44:38.247556   61932 kubeadm.go:322] [upload-certs] Skipping phase. Please see --upload-certs
I0101 15:44:38.247826   61932 kubeadm.go:322] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0101 15:44:38.769069   61932 kubeadm.go:322] [bootstrap-token] Using token: y35pec.czrb053s62slgqq7
I0101 15:44:38.808803   61932 out.go:204]     ‚ñ™ Configuring RBAC rules ...
I0101 15:44:38.809204   61932 kubeadm.go:322] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0101 15:44:38.815489   61932 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0101 15:44:38.837627   61932 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0101 15:44:38.842490   61932 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0101 15:44:38.848665   61932 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0101 15:44:38.853706   61932 kubeadm.go:322] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0101 15:44:38.865803   61932 kubeadm.go:322] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0101 15:44:39.055149   61932 kubeadm.go:322] [addons] Applied essential addon: CoreDNS
I0101 15:44:39.220638   61932 kubeadm.go:322] [addons] Applied essential addon: kube-proxy
I0101 15:44:39.221565   61932 kubeadm.go:322] 
I0101 15:44:39.221641   61932 kubeadm.go:322] Your Kubernetes control-plane has initialized successfully!
I0101 15:44:39.221647   61932 kubeadm.go:322] 
I0101 15:44:39.221722   61932 kubeadm.go:322] To start using your cluster, you need to run the following as a regular user:
I0101 15:44:39.221727   61932 kubeadm.go:322] 
I0101 15:44:39.221759   61932 kubeadm.go:322]   mkdir -p $HOME/.kube
I0101 15:44:39.221866   61932 kubeadm.go:322]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0101 15:44:39.221963   61932 kubeadm.go:322]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0101 15:44:39.221968   61932 kubeadm.go:322] 
I0101 15:44:39.222039   61932 kubeadm.go:322] Alternatively, if you are the root user, you can run:
I0101 15:44:39.222043   61932 kubeadm.go:322] 
I0101 15:44:39.222089   61932 kubeadm.go:322]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0101 15:44:39.222101   61932 kubeadm.go:322] 
I0101 15:44:39.222170   61932 kubeadm.go:322] You should now deploy a pod network to the cluster.
I0101 15:44:39.222269   61932 kubeadm.go:322] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0101 15:44:39.222329   61932 kubeadm.go:322]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0101 15:44:39.222335   61932 kubeadm.go:322] 
I0101 15:44:39.222412   61932 kubeadm.go:322] You can now join any number of control-plane nodes by copying certificate authorities
I0101 15:44:39.222481   61932 kubeadm.go:322] and service account keys on each node and then running the following as root:
I0101 15:44:39.222484   61932 kubeadm.go:322] 
I0101 15:44:39.222602   61932 kubeadm.go:322]   kubeadm join control-plane.minikube.internal:8443 --token y35pec.czrb053s62slgqq7 \
I0101 15:44:39.222741   61932 kubeadm.go:322] 	--discovery-token-ca-cert-hash sha256:8a800ca03969a09aba25e3773f1afb432c2601dc98d86e43bd1e10ac56eb0d4c \
I0101 15:44:39.222763   61932 kubeadm.go:322] 	--control-plane 
I0101 15:44:39.222774   61932 kubeadm.go:322] 
I0101 15:44:39.222875   61932 kubeadm.go:322] Then you can join any number of worker nodes by running the following on each as root:
I0101 15:44:39.222878   61932 kubeadm.go:322] 
I0101 15:44:39.222955   61932 kubeadm.go:322] kubeadm join control-plane.minikube.internal:8443 --token y35pec.czrb053s62slgqq7 \
I0101 15:44:39.223036   61932 kubeadm.go:322] 	--discovery-token-ca-cert-hash sha256:8a800ca03969a09aba25e3773f1afb432c2601dc98d86e43bd1e10ac56eb0d4c 
I0101 15:44:39.223825   61932 kubeadm.go:322] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0101 15:44:39.223838   61932 cni.go:84] Creating CNI manager for ""
I0101 15:44:39.223848   61932 cni.go:136] 1 nodes found, recommending kindnet
I0101 15:44:39.244526   61932 out.go:177] üîó  Configuring CNI (Container Networking Interface) ...
I0101 15:44:39.280754   61932 ssh_runner.go:195] Run: stat /opt/cni/bin/portmap
I0101 15:44:39.286695   61932 cni.go:182] applying CNI manifest using /var/lib/minikube/binaries/v1.28.3/kubectl ...
I0101 15:44:39.286704   61932 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (2438 bytes)
I0101 15:44:39.332224   61932 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I0101 15:44:40.105323   61932 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0101 15:44:40.105427   61932 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0101 15:44:40.105434   61932 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl label nodes minikube.k8s.io/version=v1.32.0 minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d minikube.k8s.io/name=minikube minikube.k8s.io/updated_at=2024_01_01T15_44_40_0700 minikube.k8s.io/primary=true --all --overwrite --kubeconfig=/var/lib/minikube/kubeconfig
I0101 15:44:40.229006   61932 kubeadm.go:1081] duration metric: took 123.673922ms to wait for elevateKubeSystemPrivileges.
I0101 15:44:40.229100   61932 ops.go:34] apiserver oom_adj: -16
I0101 15:44:40.229263   61932 kubeadm.go:406] StartCluster complete in 11.804373235s
I0101 15:44:40.229281   61932 settings.go:142] acquiring lock: {Name:mk9cc82adfe71567008ed5a69baec044c1f255e2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0101 15:44:40.229379   61932 settings.go:150] Updating kubeconfig:  /Users/ahmedabdo2070/.kube/config
I0101 15:44:40.233481   61932 lock.go:35] WriteFile acquiring /Users/ahmedabdo2070/.kube/config: {Name:mk98126bb6b22ee399d728d8c366610975caa028 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0101 15:44:40.233840   61932 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0101 15:44:40.233894   61932 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0101 15:44:40.233943   61932 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0101 15:44:40.233942   61932 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0101 15:44:40.233958   61932 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0101 15:44:40.233959   61932 addons.go:231] Setting addon storage-provisioner=true in "minikube"
I0101 15:44:40.233982   61932 config.go:182] Loaded profile config "minikube": Driver=hyperkit, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0101 15:44:40.234005   61932 host.go:66] Checking if "minikube" exists ...
I0101 15:44:40.234317   61932 main.go:141] libmachine: Found binary path at /Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit
I0101 15:44:40.234317   61932 main.go:141] libmachine: Found binary path at /Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit
I0101 15:44:40.234337   61932 main.go:141] libmachine: Launching plugin server for driver hyperkit
I0101 15:44:40.234343   61932 main.go:141] libmachine: Launching plugin server for driver hyperkit
I0101 15:44:40.246299   61932 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:55045
I0101 15:44:40.246329   61932 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:55044
I0101 15:44:40.246840   61932 main.go:141] libmachine: () Calling .GetVersion
I0101 15:44:40.246879   61932 main.go:141] libmachine: () Calling .GetVersion
I0101 15:44:40.247349   61932 main.go:141] libmachine: Using API Version  1
I0101 15:44:40.247366   61932 main.go:141] libmachine: () Calling .SetConfigRaw
I0101 15:44:40.247439   61932 main.go:141] libmachine: Using API Version  1
I0101 15:44:40.247451   61932 main.go:141] libmachine: () Calling .SetConfigRaw
I0101 15:44:40.247701   61932 main.go:141] libmachine: () Calling .GetMachineName
I0101 15:44:40.247848   61932 main.go:141] libmachine: () Calling .GetMachineName
I0101 15:44:40.248035   61932 main.go:141] libmachine: (minikube) Calling .GetState
I0101 15:44:40.248152   61932 main.go:141] libmachine: Found binary path at /Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit
I0101 15:44:40.248169   61932 main.go:141] libmachine: (minikube) DBG | exe=/Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0101 15:44:40.248185   61932 main.go:141] libmachine: Launching plugin server for driver hyperkit
I0101 15:44:40.248282   61932 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 61943
I0101 15:44:40.255464   61932 addons.go:231] Setting addon default-storageclass=true in "minikube"
I0101 15:44:40.255493   61932 host.go:66] Checking if "minikube" exists ...
I0101 15:44:40.255710   61932 main.go:141] libmachine: Found binary path at /Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit
I0101 15:44:40.255734   61932 main.go:141] libmachine: Launching plugin server for driver hyperkit
I0101 15:44:40.259397   61932 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:55048
I0101 15:44:40.259924   61932 main.go:141] libmachine: () Calling .GetVersion
I0101 15:44:40.260391   61932 main.go:141] libmachine: Using API Version  1
I0101 15:44:40.260403   61932 main.go:141] libmachine: () Calling .SetConfigRaw
I0101 15:44:40.260720   61932 main.go:141] libmachine: () Calling .GetMachineName
I0101 15:44:40.260875   61932 main.go:141] libmachine: (minikube) Calling .GetState
I0101 15:44:40.261029   61932 main.go:141] libmachine: (minikube) DBG | exe=/Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0101 15:44:40.261158   61932 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 61943
I0101 15:44:40.261569   61932 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0101 15:44:40.261595   61932 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.64.11 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0101 15:44:40.282851   61932 out.go:177] üîé  Verifying Kubernetes components...
I0101 15:44:40.263680   61932 main.go:141] libmachine: (minikube) Calling .DriverName
I0101 15:44:40.266365   61932 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:55050
I0101 15:44:40.301669   61932 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0101 15:44:40.320586   61932 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0101 15:44:40.302070   61932 main.go:141] libmachine: () Calling .GetVersion
I0101 15:44:40.321277   61932 main.go:141] libmachine: Using API Version  1
I0101 15:44:40.338663   61932 main.go:141] libmachine: () Calling .SetConfigRaw
I0101 15:44:40.338744   61932 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0101 15:44:40.338760   61932 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0101 15:44:40.338793   61932 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0101 15:44:40.338971   61932 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0101 15:44:40.339085   61932 main.go:141] libmachine: () Calling .GetMachineName
I0101 15:44:40.339105   61932 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0101 15:44:40.339224   61932 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0101 15:44:40.339394   61932 sshutil.go:53] new ssh client: &{IP:192.168.64.11 Port:22 SSHKeyPath:/Users/ahmedabdo2070/.minikube/machines/minikube/id_rsa Username:docker}
I0101 15:44:40.339663   61932 main.go:141] libmachine: Found binary path at /Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit
I0101 15:44:40.339681   61932 main.go:141] libmachine: Launching plugin server for driver hyperkit
I0101 15:44:40.350795   61932 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:55053
I0101 15:44:40.351590   61932 main.go:141] libmachine: () Calling .GetVersion
I0101 15:44:40.352114   61932 main.go:141] libmachine: Using API Version  1
I0101 15:44:40.352127   61932 main.go:141] libmachine: () Calling .SetConfigRaw
I0101 15:44:40.352436   61932 main.go:141] libmachine: () Calling .GetMachineName
I0101 15:44:40.352625   61932 main.go:141] libmachine: (minikube) Calling .GetState
I0101 15:44:40.352773   61932 main.go:141] libmachine: (minikube) DBG | exe=/Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0101 15:44:40.352895   61932 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 61943
I0101 15:44:40.354866   61932 main.go:141] libmachine: (minikube) Calling .DriverName
I0101 15:44:40.355076   61932 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0101 15:44:40.355081   61932 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0101 15:44:40.355090   61932 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0101 15:44:40.355196   61932 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0101 15:44:40.355310   61932 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0101 15:44:40.355434   61932 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0101 15:44:40.355574   61932 sshutil.go:53] new ssh client: &{IP:192.168.64.11 Port:22 SSHKeyPath:/Users/ahmedabdo2070/.minikube/machines/minikube/id_rsa Username:docker}
I0101 15:44:40.392069   61932 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.64.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0101 15:44:40.394496   61932 api_server.go:52] waiting for apiserver process to appear ...
I0101 15:44:40.394575   61932 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0101 15:44:40.450583   61932 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0101 15:44:40.471088   61932 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0101 15:44:40.961433   61932 start.go:926] {"host.minikube.internal": 192.168.64.1} host record injected into CoreDNS's ConfigMap
I0101 15:44:40.961474   61932 api_server.go:72] duration metric: took 699.848551ms to wait for apiserver process to appear ...
I0101 15:44:40.961487   61932 api_server.go:88] waiting for apiserver healthz status ...
I0101 15:44:40.961505   61932 api_server.go:253] Checking apiserver healthz at https://192.168.64.11:8443/healthz ...
I0101 15:44:40.966575   61932 api_server.go:279] https://192.168.64.11:8443/healthz returned 200:
ok
I0101 15:44:40.967609   61932 api_server.go:141] control plane version: v1.28.3
I0101 15:44:40.967619   61932 api_server.go:131] duration metric: took 6.129115ms to wait for apiserver health ...
I0101 15:44:40.967630   61932 system_pods.go:43] waiting for kube-system pods to appear ...
I0101 15:44:40.972638   61932 system_pods.go:59] 4 kube-system pods found
I0101 15:44:40.972655   61932 system_pods.go:61] "etcd-minikube" [5c94ea36-230f-4427-a03f-abcfe9567fbe] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0101 15:44:40.972667   61932 system_pods.go:61] "kube-apiserver-minikube" [c4982ae8-ad9f-482c-80e1-67ab3d1724ef] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0101 15:44:40.972675   61932 system_pods.go:61] "kube-controller-manager-minikube" [a5abda40-0c3e-411f-b991-1ca29eedcd45] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0101 15:44:40.972678   61932 system_pods.go:61] "kube-scheduler-minikube" [b7f57dcb-4d44-4d4b-b1ac-425c9dccee79] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0101 15:44:40.972682   61932 system_pods.go:74] duration metric: took 5.048718ms to wait for pod list to return data ...
I0101 15:44:40.972688   61932 kubeadm.go:581] duration metric: took 711.066081ms to wait for : map[apiserver:true system_pods:true] ...
I0101 15:44:40.972696   61932 node_conditions.go:102] verifying NodePressure condition ...
I0101 15:44:40.975369   61932 node_conditions.go:122] node storage ephemeral capacity is 17784752Ki
I0101 15:44:40.975384   61932 node_conditions.go:123] node cpu capacity is 2
I0101 15:44:40.975392   61932 node_conditions.go:105] duration metric: took 2.693036ms to run NodePressure ...
I0101 15:44:40.975399   61932 start.go:228] waiting for startup goroutines ...
I0101 15:44:41.124840   61932 main.go:141] libmachine: Making call to close driver server
I0101 15:44:41.124851   61932 main.go:141] libmachine: (minikube) Calling .Close
I0101 15:44:41.124853   61932 main.go:141] libmachine: Making call to close driver server
I0101 15:44:41.124861   61932 main.go:141] libmachine: (minikube) Calling .Close
I0101 15:44:41.125109   61932 main.go:141] libmachine: (minikube) DBG | Closing plugin on server side
I0101 15:44:41.125119   61932 main.go:141] libmachine: Successfully made call to close driver server
I0101 15:44:41.125133   61932 main.go:141] libmachine: Making call to close connection to plugin binary
I0101 15:44:41.125143   61932 main.go:141] libmachine: (minikube) DBG | Closing plugin on server side
I0101 15:44:41.125144   61932 main.go:141] libmachine: Making call to close driver server
I0101 15:44:41.125150   61932 main.go:141] libmachine: (minikube) Calling .Close
I0101 15:44:41.125198   61932 main.go:141] libmachine: Successfully made call to close driver server
I0101 15:44:41.125212   61932 main.go:141] libmachine: Making call to close connection to plugin binary
I0101 15:44:41.125217   61932 main.go:141] libmachine: Making call to close driver server
I0101 15:44:41.125224   61932 main.go:141] libmachine: (minikube) Calling .Close
I0101 15:44:41.125359   61932 main.go:141] libmachine: Successfully made call to close driver server
I0101 15:44:41.125365   61932 main.go:141] libmachine: Making call to close connection to plugin binary
I0101 15:44:41.125380   61932 main.go:141] libmachine: (minikube) DBG | Closing plugin on server side
I0101 15:44:41.125445   61932 main.go:141] libmachine: (minikube) DBG | Closing plugin on server side
I0101 15:44:41.125465   61932 main.go:141] libmachine: Successfully made call to close driver server
I0101 15:44:41.125470   61932 main.go:141] libmachine: Making call to close connection to plugin binary
I0101 15:44:41.132712   61932 main.go:141] libmachine: Making call to close driver server
I0101 15:44:41.132720   61932 main.go:141] libmachine: (minikube) Calling .Close
I0101 15:44:41.132894   61932 main.go:141] libmachine: Successfully made call to close driver server
I0101 15:44:41.132902   61932 main.go:141] libmachine: Making call to close connection to plugin binary
I0101 15:44:41.152734   61932 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I0101 15:44:41.191061   61932 addons.go:502] enable addons completed in 957.167909ms: enabled=[storage-provisioner default-storageclass]
I0101 15:44:41.191121   61932 start.go:233] waiting for cluster config update ...
I0101 15:44:41.191137   61932 start.go:242] writing updated cluster config ...
I0101 15:44:41.210934   61932 out.go:177] 
I0101 15:44:41.230802   61932 config.go:182] Loaded profile config "minikube": Driver=hyperkit, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0101 15:44:41.230885   61932 profile.go:148] Saving config to /Users/ahmedabdo2070/.minikube/profiles/minikube/config.json ...
I0101 15:44:41.250083   61932 out.go:177] üëç  Starting worker node minikube-m02 in cluster minikube
I0101 15:44:41.287032   61932 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0101 15:44:41.287126   61932 cache.go:56] Caching tarball of preloaded images
I0101 15:44:41.287785   61932 preload.go:174] Found /Users/ahmedabdo2070/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0101 15:44:41.287821   61932 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0101 15:44:41.288051   61932 profile.go:148] Saving config to /Users/ahmedabdo2070/.minikube/profiles/minikube/config.json ...
I0101 15:44:41.328231   61932 start.go:365] acquiring machines lock for minikube-m02: {Name:mk750fdd79c85fae9841895e2430443f1c3daa91 Clock:{} Delay:500ms Timeout:13m0s Cancel:<nil>}
I0101 15:44:41.328396   61932 start.go:369] acquired machines lock for "minikube-m02" in 137.86¬µs
I0101 15:44:41.328426   61932 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.32.1-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:8192 CPUs:2 DiskSize:20000 VMDriver: Driver:hyperkit HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.64.11 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP: Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:} &{Name:m02 IP: Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}
I0101 15:44:41.328582   61932 start.go:125] createHost starting for "m02" (driver="hyperkit")
I0101 15:44:41.366535   61932 out.go:204] üî•  Creating hyperkit VM (CPUs=2, Memory=8192MB, Disk=20000MB) ...
I0101 15:44:41.366745   61932 main.go:141] libmachine: Found binary path at /Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit
I0101 15:44:41.366777   61932 main.go:141] libmachine: Launching plugin server for driver hyperkit
I0101 15:44:41.383742   61932 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:55057
I0101 15:44:41.384333   61932 main.go:141] libmachine: () Calling .GetVersion
I0101 15:44:41.384911   61932 main.go:141] libmachine: Using API Version  1
I0101 15:44:41.384932   61932 main.go:141] libmachine: () Calling .SetConfigRaw
I0101 15:44:41.385314   61932 main.go:141] libmachine: () Calling .GetMachineName
I0101 15:44:41.385517   61932 main.go:141] libmachine: (minikube-m02) Calling .GetMachineName
I0101 15:44:41.385637   61932 main.go:141] libmachine: (minikube-m02) Calling .DriverName
I0101 15:44:41.385793   61932 start.go:159] libmachine.API.Create for "minikube" (driver="hyperkit")
I0101 15:44:41.385817   61932 client.go:168] LocalClient.Create starting
I0101 15:44:41.385865   61932 main.go:141] libmachine: Reading certificate data from /Users/ahmedabdo2070/.minikube/certs/ca.pem
I0101 15:44:41.385912   61932 main.go:141] libmachine: Decoding PEM data...
I0101 15:44:41.385925   61932 main.go:141] libmachine: Parsing certificate...
I0101 15:44:41.386003   61932 main.go:141] libmachine: Reading certificate data from /Users/ahmedabdo2070/.minikube/certs/cert.pem
I0101 15:44:41.386037   61932 main.go:141] libmachine: Decoding PEM data...
I0101 15:44:41.386046   61932 main.go:141] libmachine: Parsing certificate...
I0101 15:44:41.386060   61932 main.go:141] libmachine: Running pre-create checks...
I0101 15:44:41.386066   61932 main.go:141] libmachine: (minikube-m02) Calling .PreCreateCheck
I0101 15:44:41.386258   61932 main.go:141] libmachine: (minikube-m02) DBG | exe=/Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0101 15:44:41.386321   61932 main.go:141] libmachine: (minikube-m02) Calling .GetConfigRaw
I0101 15:44:41.386978   61932 main.go:141] libmachine: Creating machine...
I0101 15:44:41.386985   61932 main.go:141] libmachine: (minikube-m02) Calling .Create
I0101 15:44:41.387117   61932 main.go:141] libmachine: (minikube-m02) DBG | exe=/Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0101 15:44:41.387351   61932 main.go:141] libmachine: (minikube-m02) DBG | I0101 15:44:41.387096   61961 common.go:145] Making disk image using store path: /Users/ahmedabdo2070/.minikube
I0101 15:44:41.387482   61932 main.go:141] libmachine: (minikube-m02) Downloading /Users/ahmedabdo2070/.minikube/cache/boot2docker.iso from file:///Users/ahmedabdo2070/.minikube/cache/iso/amd64/minikube-v1.32.1-amd64.iso...
I0101 15:44:41.577678   61932 main.go:141] libmachine: (minikube-m02) DBG | I0101 15:44:41.577603   61961 common.go:152] Creating ssh key: /Users/ahmedabdo2070/.minikube/machines/minikube-m02/id_rsa...
I0101 15:44:41.774426   61932 main.go:141] libmachine: (minikube-m02) DBG | I0101 15:44:41.774286   61961 common.go:158] Creating raw disk image: /Users/ahmedabdo2070/.minikube/machines/minikube-m02/minikube-m02.rawdisk...
I0101 15:44:41.774437   61932 main.go:141] libmachine: (minikube-m02) DBG | Writing magic tar header
I0101 15:44:41.774447   61932 main.go:141] libmachine: (minikube-m02) DBG | Writing SSH key tar header
I0101 15:44:41.775363   61932 main.go:141] libmachine: (minikube-m02) DBG | I0101 15:44:41.775248   61961 common.go:172] Fixing permissions on /Users/ahmedabdo2070/.minikube/machines/minikube-m02 ...
I0101 15:44:42.181707   61932 main.go:141] libmachine: (minikube-m02) DBG | exe=/Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0101 15:44:42.181720   61932 main.go:141] libmachine: (minikube-m02) DBG | clean start, hyperkit pid file doesn't exist: /Users/ahmedabdo2070/.minikube/machines/minikube-m02/hyperkit.pid
I0101 15:44:42.181743   61932 main.go:141] libmachine: (minikube-m02) DBG | Using UUID 26750b34-a89b-11ee-8a75-acde48001122
I0101 15:44:42.231571   61932 main.go:141] libmachine: (minikube-m02) DBG | Generated MAC 5e:ab:2c:a3:fb:b8
I0101 15:44:42.231596   61932 main.go:141] libmachine: (minikube-m02) DBG | Starting with cmdline: loglevel=3 console=ttyS0 console=tty0 noembed nomodeset norestore waitusb=10 systemd.legacy_systemd_cgroup_controller=yes random.trust_cpu=on hw_rng_model=virtio base host=minikube
I0101 15:44:42.231652   61932 main.go:141] libmachine: (minikube-m02) DBG | 2024/01/01 15:44:42 DEBUG: hyperkit: Start &hyperkit.HyperKit{HyperKit:"/usr/local/bin/hyperkit", Argv0:"", StateDir:"/Users/ahmedabdo2070/.minikube/machines/minikube-m02", VPNKitSock:"", VPNKitUUID:"", VPNKitPreferredIPv4:"", UUID:"26750b34-a89b-11ee-8a75-acde48001122", Disks:[]hyperkit.Disk{(*hyperkit.RawDisk)(0xc00015e210)}, ISOImages:[]string{"/Users/ahmedabdo2070/.minikube/machines/minikube-m02/boot2docker.iso"}, VSock:false, VSockDir:"", VSockPorts:[]int(nil), VSockGuestCID:3, VMNet:true, Sockets9P:[]hyperkit.Socket9P(nil), Kernel:"/Users/ahmedabdo2070/.minikube/machines/minikube-m02/bzimage", Initrd:"/Users/ahmedabdo2070/.minikube/machines/minikube-m02/initrd", Bootrom:"", CPUs:2, Memory:8192, Console:1, Serials:[]hyperkit.Serial(nil), Pid:0, Arguments:[]string(nil), CmdLine:"", process:(*os.Process)(nil)}
I0101 15:44:42.231688   61932 main.go:141] libmachine: (minikube-m02) DBG | 2024/01/01 15:44:42 DEBUG: hyperkit: check &hyperkit.HyperKit{HyperKit:"/usr/local/bin/hyperkit", Argv0:"", StateDir:"/Users/ahmedabdo2070/.minikube/machines/minikube-m02", VPNKitSock:"", VPNKitUUID:"", VPNKitPreferredIPv4:"", UUID:"26750b34-a89b-11ee-8a75-acde48001122", Disks:[]hyperkit.Disk{(*hyperkit.RawDisk)(0xc00015e210)}, ISOImages:[]string{"/Users/ahmedabdo2070/.minikube/machines/minikube-m02/boot2docker.iso"}, VSock:false, VSockDir:"", VSockPorts:[]int(nil), VSockGuestCID:3, VMNet:true, Sockets9P:[]hyperkit.Socket9P(nil), Kernel:"/Users/ahmedabdo2070/.minikube/machines/minikube-m02/bzimage", Initrd:"/Users/ahmedabdo2070/.minikube/machines/minikube-m02/initrd", Bootrom:"", CPUs:2, Memory:8192, Console:1, Serials:[]hyperkit.Serial(nil), Pid:0, Arguments:[]string(nil), CmdLine:"", process:(*os.Process)(nil)}
I0101 15:44:42.231736   61932 main.go:141] libmachine: (minikube-m02) DBG | 2024/01/01 15:44:42 DEBUG: hyperkit: Arguments: []string{"-A", "-u", "-F", "/Users/ahmedabdo2070/.minikube/machines/minikube-m02/hyperkit.pid", "-c", "2", "-m", "8192M", "-s", "0:0,hostbridge", "-s", "31,lpc", "-s", "1:0,virtio-net", "-U", "26750b34-a89b-11ee-8a75-acde48001122", "-s", "2:0,virtio-blk,/Users/ahmedabdo2070/.minikube/machines/minikube-m02/minikube-m02.rawdisk", "-s", "3,ahci-cd,/Users/ahmedabdo2070/.minikube/machines/minikube-m02/boot2docker.iso", "-s", "4,virtio-rnd", "-l", "com1,autopty=/Users/ahmedabdo2070/.minikube/machines/minikube-m02/tty,log=/Users/ahmedabdo2070/.minikube/machines/minikube-m02/console-ring", "-f", "kexec,/Users/ahmedabdo2070/.minikube/machines/minikube-m02/bzimage,/Users/ahmedabdo2070/.minikube/machines/minikube-m02/initrd,earlyprintk=serial loglevel=3 console=ttyS0 console=tty0 noembed nomodeset norestore waitusb=10 systemd.legacy_systemd_cgroup_controller=yes random.trust_cpu=on hw_rng_model=virtio base host=minikube"}
I0101 15:44:42.231767   61932 main.go:141] libmachine: (minikube-m02) DBG | 2024/01/01 15:44:42 DEBUG: hyperkit: CmdLine: "/usr/local/bin/hyperkit -A -u -F /Users/ahmedabdo2070/.minikube/machines/minikube-m02/hyperkit.pid -c 2 -m 8192M -s 0:0,hostbridge -s 31,lpc -s 1:0,virtio-net -U 26750b34-a89b-11ee-8a75-acde48001122 -s 2:0,virtio-blk,/Users/ahmedabdo2070/.minikube/machines/minikube-m02/minikube-m02.rawdisk -s 3,ahci-cd,/Users/ahmedabdo2070/.minikube/machines/minikube-m02/boot2docker.iso -s 4,virtio-rnd -l com1,autopty=/Users/ahmedabdo2070/.minikube/machines/minikube-m02/tty,log=/Users/ahmedabdo2070/.minikube/machines/minikube-m02/console-ring -f kexec,/Users/ahmedabdo2070/.minikube/machines/minikube-m02/bzimage,/Users/ahmedabdo2070/.minikube/machines/minikube-m02/initrd,earlyprintk=serial loglevel=3 console=ttyS0 console=tty0 noembed nomodeset norestore waitusb=10 systemd.legacy_systemd_cgroup_controller=yes random.trust_cpu=on hw_rng_model=virtio base host=minikube"
I0101 15:44:42.231795   61932 main.go:141] libmachine: (minikube-m02) DBG | 2024/01/01 15:44:42 DEBUG: hyperkit: Redirecting stdout/stderr to logger
I0101 15:44:42.235744   61932 main.go:141] libmachine: (minikube-m02) DBG | 2024/01/01 15:44:42 DEBUG: hyperkit: Pid is 61962
I0101 15:44:42.236242   61932 main.go:141] libmachine: (minikube-m02) DBG | Attempt 0
I0101 15:44:42.236260   61932 main.go:141] libmachine: (minikube-m02) DBG | exe=/Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0101 15:44:42.236360   61932 main.go:141] libmachine: (minikube-m02) DBG | hyperkit pid from json: 61962
I0101 15:44:42.238368   61932 main.go:141] libmachine: (minikube-m02) DBG | Searching for 5e:ab:2c:a3:fb:b8 in /var/db/dhcpd_leases ...
I0101 15:44:42.238430   61932 main.go:141] libmachine: (minikube-m02) DBG | Found 10 entries in /var/db/dhcpd_leases!
I0101 15:44:42.238445   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.11 HWAddress:86:d7:9a:d6:bc:19 ID:1,86:d7:9a:d6:bc:19 Lease:0x6593f70d}
I0101 15:44:42.238457   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.10 HWAddress:ba:9a:8f:e9:3e:25 ID:1,ba:9a:8f:e9:3e:25 Lease:0x6593e37b}
I0101 15:44:42.238472   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:courageous-goose IPAddress:192.168.64.9 HWAddress:16:58:ef:f5:88:c3 ID:1,16:58:ef:f5:88:c3 Lease:0x5fe9f075}
I0101 15:44:42.238485   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:primary IPAddress:192.168.64.8 HWAddress:ca:3b:cc:e:10:69 ID:1,ca:3b:cc:e:10:69 Lease:0x5fe8b2dc}
I0101 15:44:42.238496   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:host1 IPAddress:192.168.64.7 HWAddress:1a:99:f4:ae:20:3f ID:1,1a:99:f4:ae:20:3f Lease:0x5fe7550b}
I0101 15:44:42.238517   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:node2 IPAddress:192.168.64.6 HWAddress:e6:c7:c9:68:4d:bf ID:1,e6:c7:c9:68:4d:bf Lease:0x5fe754ea}
I0101 15:44:42.238525   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:node1 IPAddress:192.168.64.5 HWAddress:42:8b:74:6d:9b:99 ID:1,42:8b:74:6d:9b:99 Lease:0x5fe754fc}
I0101 15:44:42.238537   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:control IPAddress:192.168.64.4 HWAddress:26:36:2:49:25:58 ID:1,26:36:2:49:25:58 Lease:0x5fe7550f}
I0101 15:44:42.238546   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:test1 IPAddress:192.168.64.3 HWAddress:aa:d1:73:57:5e:f3 ID:1,aa:d1:73:57:5e:f3 Lease:0x5fe6eb60}
I0101 15:44:42.238558   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:crc-ctj2r-master-0 IPAddress:192.168.64.2 HWAddress:e:f0:e3:c7:4f:be ID:1,e:f0:e3:c7:4f:be Lease:0x603213ae}
I0101 15:44:42.245283   61932 main.go:141] libmachine: (minikube-m02) DBG | 2024/01/01 15:44:42 INFO : hyperkit: stderr: Using fd 5 for I/O notifications
I0101 15:44:42.257157   61932 main.go:141] libmachine: (minikube-m02) DBG | 2024/01/01 15:44:42 INFO : hyperkit: stderr: /Users/ahmedabdo2070/.minikube/machines/minikube-m02/boot2docker.iso: fcntl(F_PUNCHHOLE) Operation not permitted: block device will not support TRIM/DISCARD
I0101 15:44:42.258101   61932 main.go:141] libmachine: (minikube-m02) DBG | 2024/01/01 15:44:42 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 2 bit: 8 unspecified don't care: bit is 0
I0101 15:44:42.258128   61932 main.go:141] libmachine: (minikube-m02) DBG | 2024/01/01 15:44:42 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 2 bit: 22 unspecified don't care: bit is 0
I0101 15:44:42.258138   61932 main.go:141] libmachine: (minikube-m02) DBG | 2024/01/01 15:44:42 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 2 bit: 25 unspecified don't care: bit is 0
I0101 15:44:42.258148   61932 main.go:141] libmachine: (minikube-m02) DBG | 2024/01/01 15:44:42 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 4 bit: 12 unspecified don't care: bit is 0
I0101 15:44:42.258160   61932 main.go:141] libmachine: (minikube-m02) DBG | 2024/01/01 15:44:42 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 4 bit: 20 unspecified don't care: bit is 0
I0101 15:44:42.258170   61932 main.go:141] libmachine: (minikube-m02) DBG | 2024/01/01 15:44:42 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 3 bit: 13 unspecified don't care: bit is 0
I0101 15:44:43.062852   61932 main.go:141] libmachine: (minikube-m02) DBG | 2024/01/01 15:44:43 INFO : hyperkit: stderr: rdmsr to register 0x3a on vcpu 0
I0101 15:44:43.062866   61932 main.go:141] libmachine: (minikube-m02) DBG | 2024/01/01 15:44:43 INFO : hyperkit: stderr: rdmsr to register 0x140 on vcpu 0
I0101 15:44:43.168669   61932 main.go:141] libmachine: (minikube-m02) DBG | 2024/01/01 15:44:43 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 2 bit: 8 unspecified don't care: bit is 0
I0101 15:44:43.168682   61932 main.go:141] libmachine: (minikube-m02) DBG | 2024/01/01 15:44:43 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 2 bit: 22 unspecified don't care: bit is 0
I0101 15:44:43.168688   61932 main.go:141] libmachine: (minikube-m02) DBG | 2024/01/01 15:44:43 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 2 bit: 25 unspecified don't care: bit is 0
I0101 15:44:43.168711   61932 main.go:141] libmachine: (minikube-m02) DBG | 2024/01/01 15:44:43 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 4 bit: 12 unspecified don't care: bit is 0
I0101 15:44:43.168731   61932 main.go:141] libmachine: (minikube-m02) DBG | 2024/01/01 15:44:43 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 4 bit: 20 unspecified don't care: bit is 0
I0101 15:44:43.168741   61932 main.go:141] libmachine: (minikube-m02) DBG | 2024/01/01 15:44:43 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 3 bit: 13 unspecified don't care: bit is 0
I0101 15:44:43.169830   61932 main.go:141] libmachine: (minikube-m02) DBG | 2024/01/01 15:44:43 INFO : hyperkit: stderr: rdmsr to register 0x3a on vcpu 1
I0101 15:44:43.169837   61932 main.go:141] libmachine: (minikube-m02) DBG | 2024/01/01 15:44:43 INFO : hyperkit: stderr: rdmsr to register 0x140 on vcpu 1
I0101 15:44:44.239640   61932 main.go:141] libmachine: (minikube-m02) DBG | Attempt 1
I0101 15:44:44.239651   61932 main.go:141] libmachine: (minikube-m02) DBG | exe=/Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0101 15:44:44.239767   61932 main.go:141] libmachine: (minikube-m02) DBG | hyperkit pid from json: 61962
I0101 15:44:44.241241   61932 main.go:141] libmachine: (minikube-m02) DBG | Searching for 5e:ab:2c:a3:fb:b8 in /var/db/dhcpd_leases ...
I0101 15:44:44.241300   61932 main.go:141] libmachine: (minikube-m02) DBG | Found 10 entries in /var/db/dhcpd_leases!
I0101 15:44:44.241315   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.11 HWAddress:86:d7:9a:d6:bc:19 ID:1,86:d7:9a:d6:bc:19 Lease:0x6593f70d}
I0101 15:44:44.241324   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.10 HWAddress:ba:9a:8f:e9:3e:25 ID:1,ba:9a:8f:e9:3e:25 Lease:0x6593e37b}
I0101 15:44:44.241330   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:courageous-goose IPAddress:192.168.64.9 HWAddress:16:58:ef:f5:88:c3 ID:1,16:58:ef:f5:88:c3 Lease:0x5fe9f075}
I0101 15:44:44.241345   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:primary IPAddress:192.168.64.8 HWAddress:ca:3b:cc:e:10:69 ID:1,ca:3b:cc:e:10:69 Lease:0x5fe8b2dc}
I0101 15:44:44.241352   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:host1 IPAddress:192.168.64.7 HWAddress:1a:99:f4:ae:20:3f ID:1,1a:99:f4:ae:20:3f Lease:0x5fe7550b}
I0101 15:44:44.241358   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:node2 IPAddress:192.168.64.6 HWAddress:e6:c7:c9:68:4d:bf ID:1,e6:c7:c9:68:4d:bf Lease:0x5fe754ea}
I0101 15:44:44.241365   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:node1 IPAddress:192.168.64.5 HWAddress:42:8b:74:6d:9b:99 ID:1,42:8b:74:6d:9b:99 Lease:0x5fe754fc}
I0101 15:44:44.241378   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:control IPAddress:192.168.64.4 HWAddress:26:36:2:49:25:58 ID:1,26:36:2:49:25:58 Lease:0x5fe7550f}
I0101 15:44:44.241386   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:test1 IPAddress:192.168.64.3 HWAddress:aa:d1:73:57:5e:f3 ID:1,aa:d1:73:57:5e:f3 Lease:0x5fe6eb60}
I0101 15:44:44.241393   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:crc-ctj2r-master-0 IPAddress:192.168.64.2 HWAddress:e:f0:e3:c7:4f:be ID:1,e:f0:e3:c7:4f:be Lease:0x603213ae}
I0101 15:44:46.241446   61932 main.go:141] libmachine: (minikube-m02) DBG | Attempt 2
I0101 15:44:46.249151   61932 main.go:141] libmachine: (minikube-m02) DBG | exe=/Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0101 15:44:46.249169   61932 main.go:141] libmachine: (minikube-m02) DBG | hyperkit pid from json: 61962
I0101 15:44:46.249180   61932 main.go:141] libmachine: (minikube-m02) DBG | Searching for 5e:ab:2c:a3:fb:b8 in /var/db/dhcpd_leases ...
I0101 15:44:46.249186   61932 main.go:141] libmachine: (minikube-m02) DBG | Found 10 entries in /var/db/dhcpd_leases!
I0101 15:44:46.249191   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.11 HWAddress:86:d7:9a:d6:bc:19 ID:1,86:d7:9a:d6:bc:19 Lease:0x6593f70d}
I0101 15:44:46.249196   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.10 HWAddress:ba:9a:8f:e9:3e:25 ID:1,ba:9a:8f:e9:3e:25 Lease:0x6593e37b}
I0101 15:44:46.249202   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:courageous-goose IPAddress:192.168.64.9 HWAddress:16:58:ef:f5:88:c3 ID:1,16:58:ef:f5:88:c3 Lease:0x5fe9f075}
I0101 15:44:46.249207   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:primary IPAddress:192.168.64.8 HWAddress:ca:3b:cc:e:10:69 ID:1,ca:3b:cc:e:10:69 Lease:0x5fe8b2dc}
I0101 15:44:46.249213   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:host1 IPAddress:192.168.64.7 HWAddress:1a:99:f4:ae:20:3f ID:1,1a:99:f4:ae:20:3f Lease:0x5fe7550b}
I0101 15:44:46.249220   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:node2 IPAddress:192.168.64.6 HWAddress:e6:c7:c9:68:4d:bf ID:1,e6:c7:c9:68:4d:bf Lease:0x5fe754ea}
I0101 15:44:46.249258   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:node1 IPAddress:192.168.64.5 HWAddress:42:8b:74:6d:9b:99 ID:1,42:8b:74:6d:9b:99 Lease:0x5fe754fc}
I0101 15:44:46.249265   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:control IPAddress:192.168.64.4 HWAddress:26:36:2:49:25:58 ID:1,26:36:2:49:25:58 Lease:0x5fe7550f}
I0101 15:44:46.249271   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:test1 IPAddress:192.168.64.3 HWAddress:aa:d1:73:57:5e:f3 ID:1,aa:d1:73:57:5e:f3 Lease:0x5fe6eb60}
I0101 15:44:46.249278   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:crc-ctj2r-master-0 IPAddress:192.168.64.2 HWAddress:e:f0:e3:c7:4f:be ID:1,e:f0:e3:c7:4f:be Lease:0x603213ae}
I0101 15:44:48.243941   61932 main.go:141] libmachine: (minikube-m02) DBG | Attempt 3
I0101 15:44:48.243956   61932 main.go:141] libmachine: (minikube-m02) DBG | exe=/Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0101 15:44:48.244014   61932 main.go:141] libmachine: (minikube-m02) DBG | hyperkit pid from json: 61962
I0101 15:44:48.245668   61932 main.go:141] libmachine: (minikube-m02) DBG | Searching for 5e:ab:2c:a3:fb:b8 in /var/db/dhcpd_leases ...
I0101 15:44:48.245736   61932 main.go:141] libmachine: (minikube-m02) DBG | Found 10 entries in /var/db/dhcpd_leases!
I0101 15:44:48.245747   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.11 HWAddress:86:d7:9a:d6:bc:19 ID:1,86:d7:9a:d6:bc:19 Lease:0x6593f70d}
I0101 15:44:48.245756   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.10 HWAddress:ba:9a:8f:e9:3e:25 ID:1,ba:9a:8f:e9:3e:25 Lease:0x6593e37b}
I0101 15:44:48.245762   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:courageous-goose IPAddress:192.168.64.9 HWAddress:16:58:ef:f5:88:c3 ID:1,16:58:ef:f5:88:c3 Lease:0x5fe9f075}
I0101 15:44:48.245768   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:primary IPAddress:192.168.64.8 HWAddress:ca:3b:cc:e:10:69 ID:1,ca:3b:cc:e:10:69 Lease:0x5fe8b2dc}
I0101 15:44:48.245773   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:host1 IPAddress:192.168.64.7 HWAddress:1a:99:f4:ae:20:3f ID:1,1a:99:f4:ae:20:3f Lease:0x5fe7550b}
I0101 15:44:48.245816   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:node2 IPAddress:192.168.64.6 HWAddress:e6:c7:c9:68:4d:bf ID:1,e6:c7:c9:68:4d:bf Lease:0x5fe754ea}
I0101 15:44:48.245825   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:node1 IPAddress:192.168.64.5 HWAddress:42:8b:74:6d:9b:99 ID:1,42:8b:74:6d:9b:99 Lease:0x5fe754fc}
I0101 15:44:48.245831   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:control IPAddress:192.168.64.4 HWAddress:26:36:2:49:25:58 ID:1,26:36:2:49:25:58 Lease:0x5fe7550f}
I0101 15:44:48.245836   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:test1 IPAddress:192.168.64.3 HWAddress:aa:d1:73:57:5e:f3 ID:1,aa:d1:73:57:5e:f3 Lease:0x5fe6eb60}
I0101 15:44:48.245856   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:crc-ctj2r-master-0 IPAddress:192.168.64.2 HWAddress:e:f0:e3:c7:4f:be ID:1,e:f0:e3:c7:4f:be Lease:0x603213ae}
I0101 15:44:48.947889   61932 main.go:141] libmachine: (minikube-m02) DBG | 2024/01/01 15:44:48 INFO : hyperkit: stderr: rdmsr to register 0x64d on vcpu 1
I0101 15:44:48.947983   61932 main.go:141] libmachine: (minikube-m02) DBG | 2024/01/01 15:44:48 INFO : hyperkit: stderr: rdmsr to register 0x64e on vcpu 1
I0101 15:44:48.947990   61932 main.go:141] libmachine: (minikube-m02) DBG | 2024/01/01 15:44:48 INFO : hyperkit: stderr: rdmsr to register 0x34 on vcpu 1
I0101 15:44:50.245945   61932 main.go:141] libmachine: (minikube-m02) DBG | Attempt 4
I0101 15:44:50.245967   61932 main.go:141] libmachine: (minikube-m02) DBG | exe=/Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0101 15:44:50.246102   61932 main.go:141] libmachine: (minikube-m02) DBG | hyperkit pid from json: 61962
I0101 15:44:50.248195   61932 main.go:141] libmachine: (minikube-m02) DBG | Searching for 5e:ab:2c:a3:fb:b8 in /var/db/dhcpd_leases ...
I0101 15:44:50.248224   61932 main.go:141] libmachine: (minikube-m02) DBG | Found 10 entries in /var/db/dhcpd_leases!
I0101 15:44:50.248233   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.11 HWAddress:86:d7:9a:d6:bc:19 ID:1,86:d7:9a:d6:bc:19 Lease:0x6593f70d}
I0101 15:44:50.248251   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.10 HWAddress:ba:9a:8f:e9:3e:25 ID:1,ba:9a:8f:e9:3e:25 Lease:0x6593e37b}
I0101 15:44:50.248259   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:courageous-goose IPAddress:192.168.64.9 HWAddress:16:58:ef:f5:88:c3 ID:1,16:58:ef:f5:88:c3 Lease:0x5fe9f075}
I0101 15:44:50.248270   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:primary IPAddress:192.168.64.8 HWAddress:ca:3b:cc:e:10:69 ID:1,ca:3b:cc:e:10:69 Lease:0x5fe8b2dc}
I0101 15:44:50.248278   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:host1 IPAddress:192.168.64.7 HWAddress:1a:99:f4:ae:20:3f ID:1,1a:99:f4:ae:20:3f Lease:0x5fe7550b}
I0101 15:44:50.248296   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:node2 IPAddress:192.168.64.6 HWAddress:e6:c7:c9:68:4d:bf ID:1,e6:c7:c9:68:4d:bf Lease:0x5fe754ea}
I0101 15:44:50.248304   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:node1 IPAddress:192.168.64.5 HWAddress:42:8b:74:6d:9b:99 ID:1,42:8b:74:6d:9b:99 Lease:0x5fe754fc}
I0101 15:44:50.248312   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:control IPAddress:192.168.64.4 HWAddress:26:36:2:49:25:58 ID:1,26:36:2:49:25:58 Lease:0x5fe7550f}
I0101 15:44:50.248317   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:test1 IPAddress:192.168.64.3 HWAddress:aa:d1:73:57:5e:f3 ID:1,aa:d1:73:57:5e:f3 Lease:0x5fe6eb60}
I0101 15:44:50.248328   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:crc-ctj2r-master-0 IPAddress:192.168.64.2 HWAddress:e:f0:e3:c7:4f:be ID:1,e:f0:e3:c7:4f:be Lease:0x603213ae}
I0101 15:44:52.248761   61932 main.go:141] libmachine: (minikube-m02) DBG | Attempt 5
I0101 15:44:52.248776   61932 main.go:141] libmachine: (minikube-m02) DBG | exe=/Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0101 15:44:52.248865   61932 main.go:141] libmachine: (minikube-m02) DBG | hyperkit pid from json: 61962
I0101 15:44:52.250358   61932 main.go:141] libmachine: (minikube-m02) DBG | Searching for 5e:ab:2c:a3:fb:b8 in /var/db/dhcpd_leases ...
I0101 15:44:52.250400   61932 main.go:141] libmachine: (minikube-m02) DBG | Found 10 entries in /var/db/dhcpd_leases!
I0101 15:44:52.250438   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.11 HWAddress:86:d7:9a:d6:bc:19 ID:1,86:d7:9a:d6:bc:19 Lease:0x6593f70d}
I0101 15:44:52.250461   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.10 HWAddress:ba:9a:8f:e9:3e:25 ID:1,ba:9a:8f:e9:3e:25 Lease:0x6593e37b}
I0101 15:44:52.250469   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:courageous-goose IPAddress:192.168.64.9 HWAddress:16:58:ef:f5:88:c3 ID:1,16:58:ef:f5:88:c3 Lease:0x5fe9f075}
I0101 15:44:52.250476   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:primary IPAddress:192.168.64.8 HWAddress:ca:3b:cc:e:10:69 ID:1,ca:3b:cc:e:10:69 Lease:0x5fe8b2dc}
I0101 15:44:52.250483   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:host1 IPAddress:192.168.64.7 HWAddress:1a:99:f4:ae:20:3f ID:1,1a:99:f4:ae:20:3f Lease:0x5fe7550b}
I0101 15:44:52.250491   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:node2 IPAddress:192.168.64.6 HWAddress:e6:c7:c9:68:4d:bf ID:1,e6:c7:c9:68:4d:bf Lease:0x5fe754ea}
I0101 15:44:52.250498   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:node1 IPAddress:192.168.64.5 HWAddress:42:8b:74:6d:9b:99 ID:1,42:8b:74:6d:9b:99 Lease:0x5fe754fc}
I0101 15:44:52.250504   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:control IPAddress:192.168.64.4 HWAddress:26:36:2:49:25:58 ID:1,26:36:2:49:25:58 Lease:0x5fe7550f}
I0101 15:44:52.250509   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:test1 IPAddress:192.168.64.3 HWAddress:aa:d1:73:57:5e:f3 ID:1,aa:d1:73:57:5e:f3 Lease:0x5fe6eb60}
I0101 15:44:52.250516   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:crc-ctj2r-master-0 IPAddress:192.168.64.2 HWAddress:e:f0:e3:c7:4f:be ID:1,e:f0:e3:c7:4f:be Lease:0x603213ae}
I0101 15:44:54.250590   61932 main.go:141] libmachine: (minikube-m02) DBG | Attempt 6
I0101 15:44:54.250607   61932 main.go:141] libmachine: (minikube-m02) DBG | exe=/Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0101 15:44:54.250772   61932 main.go:141] libmachine: (minikube-m02) DBG | hyperkit pid from json: 61962
I0101 15:44:54.253416   61932 main.go:141] libmachine: (minikube-m02) DBG | Searching for 5e:ab:2c:a3:fb:b8 in /var/db/dhcpd_leases ...
I0101 15:44:54.253553   61932 main.go:141] libmachine: (minikube-m02) DBG | Found 11 entries in /var/db/dhcpd_leases!
I0101 15:44:54.253565   61932 main.go:141] libmachine: (minikube-m02) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.12 HWAddress:5e:ab:2c:a3:fb:b8 ID:1,5e:ab:2c:a3:fb:b8 Lease:0x6593f734}
I0101 15:44:54.253605   61932 main.go:141] libmachine: (minikube-m02) DBG | Found match: 5e:ab:2c:a3:fb:b8
I0101 15:44:54.253620   61932 main.go:141] libmachine: (minikube-m02) DBG | IP: 192.168.64.12
I0101 15:44:54.253675   61932 main.go:141] libmachine: (minikube-m02) Calling .GetConfigRaw
I0101 15:44:54.254910   61932 main.go:141] libmachine: (minikube-m02) Calling .DriverName
I0101 15:44:54.255180   61932 main.go:141] libmachine: (minikube-m02) Calling .DriverName
I0101 15:44:54.255367   61932 main.go:141] libmachine: Waiting for machine to be running, this may take a few minutes...
I0101 15:44:54.255379   61932 main.go:141] libmachine: (minikube-m02) Calling .GetState
I0101 15:44:54.255561   61932 main.go:141] libmachine: (minikube-m02) DBG | exe=/Users/ahmedabdo2070/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0101 15:44:54.255681   61932 main.go:141] libmachine: (minikube-m02) DBG | hyperkit pid from json: 61962
I0101 15:44:54.258502   61932 main.go:141] libmachine: Detecting operating system of created instance...
I0101 15:44:54.258512   61932 main.go:141] libmachine: Waiting for SSH to be available...
I0101 15:44:54.258518   61932 main.go:141] libmachine: Getting to WaitForSSH function...
I0101 15:44:54.258525   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHHostname
I0101 15:44:54.258690   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHPort
I0101 15:44:54.258905   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHKeyPath
I0101 15:44:54.259098   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHKeyPath
I0101 15:44:54.259314   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHUsername
I0101 15:44:54.259591   61932 main.go:141] libmachine: Using SSH client type: native
I0101 15:44:54.260648   61932 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10040a380] 0x10040d060 <nil>  [] 0s} 192.168.64.12 22 <nil> <nil>}
I0101 15:44:54.260655   61932 main.go:141] libmachine: About to run SSH command:
exit 0
I0101 15:44:54.345411   61932 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0101 15:44:54.345420   61932 main.go:141] libmachine: Detecting the provisioner...
I0101 15:44:54.345436   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHHostname
I0101 15:44:54.345612   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHPort
I0101 15:44:54.345705   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHKeyPath
I0101 15:44:54.345813   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHKeyPath
I0101 15:44:54.345907   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHUsername
I0101 15:44:54.346073   61932 main.go:141] libmachine: Using SSH client type: native
I0101 15:44:54.346395   61932 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10040a380] 0x10040d060 <nil>  [] 0s} 192.168.64.12 22 <nil> <nil>}
I0101 15:44:54.346400   61932 main.go:141] libmachine: About to run SSH command:
cat /etc/os-release
I0101 15:44:54.429527   61932 main.go:141] libmachine: SSH cmd err, output: <nil>: NAME=Buildroot
VERSION=2021.02.12-1-gb75713b-dirty
ID=buildroot
VERSION_ID=2021.02.12
PRETTY_NAME="Buildroot 2021.02.12"

I0101 15:44:54.429613   61932 main.go:141] libmachine: found compatible host: buildroot
I0101 15:44:54.429622   61932 main.go:141] libmachine: Provisioning with buildroot...
I0101 15:44:54.429629   61932 main.go:141] libmachine: (minikube-m02) Calling .GetMachineName
I0101 15:44:54.429875   61932 buildroot.go:166] provisioning hostname "minikube-m02"
I0101 15:44:54.429887   61932 main.go:141] libmachine: (minikube-m02) Calling .GetMachineName
I0101 15:44:54.430030   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHHostname
I0101 15:44:54.430149   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHPort
I0101 15:44:54.430309   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHKeyPath
I0101 15:44:54.430440   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHKeyPath
I0101 15:44:54.430586   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHUsername
I0101 15:44:54.431021   61932 main.go:141] libmachine: Using SSH client type: native
I0101 15:44:54.431488   61932 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10040a380] 0x10040d060 <nil>  [] 0s} 192.168.64.12 22 <nil> <nil>}
I0101 15:44:54.431496   61932 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube-m02 && echo "minikube-m02" | sudo tee /etc/hostname
I0101 15:44:54.527694   61932 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube-m02

I0101 15:44:54.527706   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHHostname
I0101 15:44:54.527857   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHPort
I0101 15:44:54.527972   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHKeyPath
I0101 15:44:54.528074   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHKeyPath
I0101 15:44:54.528156   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHUsername
I0101 15:44:54.528325   61932 main.go:141] libmachine: Using SSH client type: native
I0101 15:44:54.528650   61932 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10040a380] 0x10040d060 <nil>  [] 0s} 192.168.64.12 22 <nil> <nil>}
I0101 15:44:54.528661   61932 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube-m02' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube-m02/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube-m02' | sudo tee -a /etc/hosts; 
			fi
		fi
I0101 15:44:54.624403   61932 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0101 15:44:54.624417   61932 buildroot.go:172] set auth options {CertDir:/Users/ahmedabdo2070/.minikube CaCertPath:/Users/ahmedabdo2070/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/ahmedabdo2070/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/ahmedabdo2070/.minikube/machines/server.pem ServerKeyPath:/Users/ahmedabdo2070/.minikube/machines/server-key.pem ClientKeyPath:/Users/ahmedabdo2070/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/ahmedabdo2070/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/ahmedabdo2070/.minikube}
I0101 15:44:54.624427   61932 buildroot.go:174] setting up certificates
I0101 15:44:54.624436   61932 provision.go:83] configureAuth start
I0101 15:44:54.624442   61932 main.go:141] libmachine: (minikube-m02) Calling .GetMachineName
I0101 15:44:54.624695   61932 main.go:141] libmachine: (minikube-m02) Calling .GetIP
I0101 15:44:54.624827   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHHostname
I0101 15:44:54.624974   61932 provision.go:138] copyHostCerts
I0101 15:44:54.625054   61932 exec_runner.go:144] found /Users/ahmedabdo2070/.minikube/ca.pem, removing ...
I0101 15:44:54.625059   61932 exec_runner.go:203] rm: /Users/ahmedabdo2070/.minikube/ca.pem
I0101 15:44:54.625215   61932 exec_runner.go:151] cp: /Users/ahmedabdo2070/.minikube/certs/ca.pem --> /Users/ahmedabdo2070/.minikube/ca.pem (1099 bytes)
I0101 15:44:54.625513   61932 exec_runner.go:144] found /Users/ahmedabdo2070/.minikube/cert.pem, removing ...
I0101 15:44:54.625518   61932 exec_runner.go:203] rm: /Users/ahmedabdo2070/.minikube/cert.pem
I0101 15:44:54.625647   61932 exec_runner.go:151] cp: /Users/ahmedabdo2070/.minikube/certs/cert.pem --> /Users/ahmedabdo2070/.minikube/cert.pem (1139 bytes)
I0101 15:44:54.625880   61932 exec_runner.go:144] found /Users/ahmedabdo2070/.minikube/key.pem, removing ...
I0101 15:44:54.625885   61932 exec_runner.go:203] rm: /Users/ahmedabdo2070/.minikube/key.pem
I0101 15:44:54.625987   61932 exec_runner.go:151] cp: /Users/ahmedabdo2070/.minikube/certs/key.pem --> /Users/ahmedabdo2070/.minikube/key.pem (1675 bytes)
I0101 15:44:54.626201   61932 provision.go:112] generating server cert: /Users/ahmedabdo2070/.minikube/machines/server.pem ca-key=/Users/ahmedabdo2070/.minikube/certs/ca.pem private-key=/Users/ahmedabdo2070/.minikube/certs/ca-key.pem org=ahmedabdo2070.minikube-m02 san=[192.168.64.12 192.168.64.12 localhost 127.0.0.1 minikube minikube-m02]
I0101 15:44:54.713041   61932 provision.go:172] copyRemoteCerts
I0101 15:44:54.713116   61932 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0101 15:44:54.713131   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHHostname
I0101 15:44:54.713350   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHPort
I0101 15:44:54.713447   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHKeyPath
I0101 15:44:54.713579   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHUsername
I0101 15:44:54.713700   61932 sshutil.go:53] new ssh client: &{IP:192.168.64.12 Port:22 SSHKeyPath:/Users/ahmedabdo2070/.minikube/machines/minikube-m02/id_rsa Username:docker}
I0101 15:44:54.761793   61932 ssh_runner.go:362] scp /Users/ahmedabdo2070/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1099 bytes)
I0101 15:44:54.782789   61932 ssh_runner.go:362] scp /Users/ahmedabdo2070/.minikube/machines/server.pem --> /etc/docker/server.pem (1229 bytes)
I0101 15:44:54.803518   61932 ssh_runner.go:362] scp /Users/ahmedabdo2070/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0101 15:44:54.821938   61932 provision.go:86] duration metric: configureAuth took 197.492961ms
I0101 15:44:54.821951   61932 buildroot.go:189] setting minikube options for container-runtime
I0101 15:44:54.822499   61932 config.go:182] Loaded profile config "minikube": Driver=hyperkit, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0101 15:44:54.822516   61932 main.go:141] libmachine: (minikube-m02) Calling .DriverName
I0101 15:44:54.822686   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHHostname
I0101 15:44:54.822783   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHPort
I0101 15:44:54.822865   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHKeyPath
I0101 15:44:54.822949   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHKeyPath
I0101 15:44:54.823034   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHUsername
I0101 15:44:54.823175   61932 main.go:141] libmachine: Using SSH client type: native
I0101 15:44:54.823455   61932 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10040a380] 0x10040d060 <nil>  [] 0s} 192.168.64.12 22 <nil> <nil>}
I0101 15:44:54.823460   61932 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0101 15:44:54.905302   61932 main.go:141] libmachine: SSH cmd err, output: <nil>: tmpfs

I0101 15:44:54.905312   61932 buildroot.go:70] root file system type: tmpfs
I0101 15:44:54.905412   61932 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0101 15:44:54.905423   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHHostname
I0101 15:44:54.905679   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHPort
I0101 15:44:54.905862   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHKeyPath
I0101 15:44:54.906028   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHKeyPath
I0101 15:44:54.906165   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHUsername
I0101 15:44:54.906399   61932 main.go:141] libmachine: Using SSH client type: native
I0101 15:44:54.906775   61932 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10040a380] 0x10040d060 <nil>  [] 0s} 192.168.64.12 22 <nil> <nil>}
I0101 15:44:54.906838   61932 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="NO_PROXY=192.168.64.11"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=hyperkit --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0101 15:44:55.006320   61932 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=NO_PROXY=192.168.64.11


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=hyperkit --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0101 15:44:55.006335   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHHostname
I0101 15:44:55.006555   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHPort
I0101 15:44:55.006744   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHKeyPath
I0101 15:44:55.006864   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHKeyPath
I0101 15:44:55.006951   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHUsername
I0101 15:44:55.007158   61932 main.go:141] libmachine: Using SSH client type: native
I0101 15:44:55.007463   61932 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10040a380] 0x10040d060 <nil>  [] 0s} 192.168.64.12 22 <nil> <nil>}
I0101 15:44:55.007473   61932 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0101 15:44:55.598433   61932 main.go:141] libmachine: SSH cmd err, output: <nil>: diff: can't stat '/lib/systemd/system/docker.service': No such file or directory
Created symlink /etc/systemd/system/multi-user.target.wants/docker.service ‚Üí /usr/lib/systemd/system/docker.service.

I0101 15:44:55.598444   61932 main.go:141] libmachine: Checking connection to Docker...
I0101 15:44:55.598452   61932 main.go:141] libmachine: (minikube-m02) Calling .GetURL
I0101 15:44:55.598622   61932 main.go:141] libmachine: Docker is up and running!
I0101 15:44:55.598628   61932 main.go:141] libmachine: Reticulating splines...
I0101 15:44:55.598632   61932 client.go:171] LocalClient.Create took 14.21275287s
I0101 15:44:55.598645   61932 start.go:167] duration metric: libmachine.API.Create for "minikube" took 14.212796017s
I0101 15:44:55.598649   61932 start.go:300] post-start starting for "minikube-m02" (driver="hyperkit")
I0101 15:44:55.598654   61932 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0101 15:44:55.598662   61932 main.go:141] libmachine: (minikube-m02) Calling .DriverName
I0101 15:44:55.598862   61932 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0101 15:44:55.598874   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHHostname
I0101 15:44:55.598979   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHPort
I0101 15:44:55.599078   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHKeyPath
I0101 15:44:55.599156   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHUsername
I0101 15:44:55.599238   61932 sshutil.go:53] new ssh client: &{IP:192.168.64.12 Port:22 SSHKeyPath:/Users/ahmedabdo2070/.minikube/machines/minikube-m02/id_rsa Username:docker}
I0101 15:44:55.650123   61932 ssh_runner.go:195] Run: cat /etc/os-release
I0101 15:44:55.654128   61932 info.go:137] Remote host: Buildroot 2021.02.12
I0101 15:44:55.654139   61932 filesync.go:126] Scanning /Users/ahmedabdo2070/.minikube/addons for local assets ...
I0101 15:44:55.654299   61932 filesync.go:126] Scanning /Users/ahmedabdo2070/.minikube/files for local assets ...
I0101 15:44:55.654407   61932 start.go:303] post-start completed in 55.751305ms
I0101 15:44:55.654435   61932 main.go:141] libmachine: (minikube-m02) Calling .GetConfigRaw
I0101 15:44:55.655112   61932 main.go:141] libmachine: (minikube-m02) Calling .GetIP
I0101 15:44:55.655357   61932 profile.go:148] Saving config to /Users/ahmedabdo2070/.minikube/profiles/minikube/config.json ...
I0101 15:44:55.655771   61932 start.go:128] duration metric: createHost completed in 14.327119745s
I0101 15:44:55.655787   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHHostname
I0101 15:44:55.655990   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHPort
I0101 15:44:55.656174   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHKeyPath
I0101 15:44:55.656325   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHKeyPath
I0101 15:44:55.656450   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHUsername
I0101 15:44:55.656618   61932 main.go:141] libmachine: Using SSH client type: native
I0101 15:44:55.656944   61932 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10040a380] 0x10040d060 <nil>  [] 0s} 192.168.64.12 22 <nil> <nil>}
I0101 15:44:55.656949   61932 main.go:141] libmachine: About to run SSH command:
date +%!s(MISSING).%!N(MISSING)
I0101 15:44:55.741156   61932 main.go:141] libmachine: SSH cmd err, output: <nil>: 1704109495.749776715

I0101 15:44:55.741166   61932 fix.go:206] guest clock: 1704109495.749776715
I0101 15:44:55.741182   61932 fix.go:219] Guest: 2024-01-01 15:44:55.749776715 +0400 +04 Remote: 2024-01-01 15:44:55.655778 +0400 +04 m=+54.491841671 (delta=93.998715ms)
I0101 15:44:55.741192   61932 fix.go:190] guest clock delta is within tolerance: 93.998715ms
I0101 15:44:55.741197   61932 start.go:83] releasing machines lock for "minikube-m02", held for 14.412735346s
I0101 15:44:55.741213   61932 main.go:141] libmachine: (minikube-m02) Calling .DriverName
I0101 15:44:55.741443   61932 main.go:141] libmachine: (minikube-m02) Calling .GetIP
I0101 15:44:55.765901   61932 out.go:177] üåê  Found network options:
I0101 15:44:55.785304   61932 out.go:177]     ‚ñ™ NO_PROXY=192.168.64.11
W0101 15:44:55.805539   61932 proxy.go:119] fail to check proxy env: Error ip not in block
I0101 15:44:55.805576   61932 main.go:141] libmachine: (minikube-m02) Calling .DriverName
I0101 15:44:55.806269   61932 main.go:141] libmachine: (minikube-m02) Calling .DriverName
I0101 15:44:55.806489   61932 main.go:141] libmachine: (minikube-m02) Calling .DriverName
W0101 15:44:55.806750   61932 proxy.go:119] fail to check proxy env: Error ip not in block
I0101 15:44:55.806785   61932 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0101 15:44:55.806808   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHHostname
I0101 15:44:55.806882   61932 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0101 15:44:55.806894   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHHostname
I0101 15:44:55.807009   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHPort
I0101 15:44:55.807052   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHPort
I0101 15:44:55.807169   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHKeyPath
I0101 15:44:55.807178   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHKeyPath
I0101 15:44:55.807342   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHUsername
I0101 15:44:55.807373   61932 main.go:141] libmachine: (minikube-m02) Calling .GetSSHUsername
I0101 15:44:55.807511   61932 sshutil.go:53] new ssh client: &{IP:192.168.64.12 Port:22 SSHKeyPath:/Users/ahmedabdo2070/.minikube/machines/minikube-m02/id_rsa Username:docker}
I0101 15:44:55.807526   61932 sshutil.go:53] new ssh client: &{IP:192.168.64.12 Port:22 SSHKeyPath:/Users/ahmedabdo2070/.minikube/machines/minikube-m02/id_rsa Username:docker}
W0101 15:44:55.853363   61932 cni.go:209] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I0101 15:44:55.853491   61932 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0101 15:44:56.456639   61932 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0101 15:44:56.456653   61932 start.go:472] detecting cgroup driver to use...
I0101 15:44:56.456759   61932 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0101 15:44:56.471455   61932 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0101 15:44:56.479174   61932 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0101 15:44:56.486695   61932 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0101 15:44:56.486794   61932 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0101 15:44:56.494340   61932 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0101 15:44:56.502619   61932 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0101 15:44:56.510078   61932 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0101 15:44:56.517913   61932 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0101 15:44:56.526031   61932 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0101 15:44:56.533603   61932 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0101 15:44:56.540355   61932 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0101 15:44:56.547155   61932 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0101 15:44:56.634919   61932 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0101 15:44:56.649293   61932 start.go:472] detecting cgroup driver to use...
I0101 15:44:56.649400   61932 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0101 15:44:56.664113   61932 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0101 15:44:56.679896   61932 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I0101 15:44:56.699852   61932 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0101 15:44:56.709596   61932 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0101 15:44:56.719668   61932 ssh_runner.go:195] Run: sudo systemctl stop -f crio
I0101 15:44:56.790735   61932 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0101 15:44:56.801226   61932 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0101 15:44:56.815713   61932 ssh_runner.go:195] Run: which cri-dockerd
I0101 15:44:56.818488   61932 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0101 15:44:56.824897   61932 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0101 15:44:56.837665   61932 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0101 15:44:56.925542   61932 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0101 15:44:57.021156   61932 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I0101 15:44:57.021180   61932 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0101 15:44:57.035427   61932 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0101 15:44:57.131373   61932 ssh_runner.go:195] Run: sudo systemctl restart docker
I0101 15:44:58.491542   61932 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.360143462s)
I0101 15:44:58.491654   61932 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0101 15:44:58.580076   61932 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0101 15:44:58.672583   61932 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0101 15:44:58.762184   61932 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0101 15:44:58.856283   61932 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0101 15:44:58.892814   61932 out.go:177] 
W0101 15:44:58.911480   61932 out.go:239] ‚ùå  Exiting due to RUNTIME_ENABLE: Failed to enable container runtime: sudo systemctl restart cri-docker.socket: Process exited with status 1
stdout:

stderr:
Job failed. See "journalctl -xe" for details.

W0101 15:44:58.911552   61932 out.go:239] 
W0101 15:44:58.913238   61932 out.go:239] [31m‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚îÇ[0m    üòø  If the above advice does not help, please let us know:                             [31m‚îÇ[0m
[31m‚îÇ[0m    üëâ  https://github.com/kubernetes/minikube/issues/new/choose                           [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚îÇ[0m    Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ[0m
I0101 15:44:58.973253   61932 out.go:177] 

* 
* ==> Docker <==
* -- Journal begins at Mon 2024-01-01 11:44:12 UTC, ends at Mon 2024-01-01 11:45:20 UTC. --
Jan 01 11:44:32 minikube dockerd[1226]: time="2024-01-01T11:44:32.393482502Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Jan 01 11:44:32 minikube dockerd[1226]: time="2024-01-01T11:44:32.393491049Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jan 01 11:44:32 minikube cri-dockerd[1111]: time="2024-01-01T11:44:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6bbb9873725dc9f9deb7f22c548911123063fcd85848bbbd717c2728503b26d2/resolv.conf as [nameserver 192.168.64.1]"
Jan 01 11:44:32 minikube dockerd[1226]: time="2024-01-01T11:44:32.892722216Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Jan 01 11:44:32 minikube dockerd[1226]: time="2024-01-01T11:44:32.892799354Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jan 01 11:44:32 minikube dockerd[1226]: time="2024-01-01T11:44:32.892821256Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Jan 01 11:44:32 minikube dockerd[1226]: time="2024-01-01T11:44:32.892832658Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jan 01 11:44:32 minikube cri-dockerd[1111]: time="2024-01-01T11:44:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7e81f325d78bbf2619619f5957af97030a1188dda3993bea056ebf639206ec6b/resolv.conf as [nameserver 192.168.64.1]"
Jan 01 11:44:33 minikube dockerd[1226]: time="2024-01-01T11:44:33.077982572Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Jan 01 11:44:33 minikube dockerd[1226]: time="2024-01-01T11:44:33.078106435Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jan 01 11:44:33 minikube dockerd[1226]: time="2024-01-01T11:44:33.078118981Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Jan 01 11:44:33 minikube dockerd[1226]: time="2024-01-01T11:44:33.078128229Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jan 01 11:44:33 minikube cri-dockerd[1111]: time="2024-01-01T11:44:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/120d5ffee4215f7994121b2f00c80bc261cc47126d9bdfef585a7fb44d189114/resolv.conf as [nameserver 192.168.64.1]"
Jan 01 11:44:33 minikube cri-dockerd[1111]: time="2024-01-01T11:44:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7b6b005ba81ee408ff9ed9db34aa25fc95264a33ae4353a2c89d2ec7f50bc645/resolv.conf as [nameserver 192.168.64.1]"
Jan 01 11:44:33 minikube dockerd[1226]: time="2024-01-01T11:44:33.242081113Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Jan 01 11:44:33 minikube dockerd[1226]: time="2024-01-01T11:44:33.242187195Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jan 01 11:44:33 minikube dockerd[1226]: time="2024-01-01T11:44:33.242206823Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Jan 01 11:44:33 minikube dockerd[1226]: time="2024-01-01T11:44:33.242214851Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jan 01 11:44:33 minikube dockerd[1226]: time="2024-01-01T11:44:33.417937391Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Jan 01 11:44:33 minikube dockerd[1226]: time="2024-01-01T11:44:33.418017714Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jan 01 11:44:33 minikube dockerd[1226]: time="2024-01-01T11:44:33.418032454Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Jan 01 11:44:33 minikube dockerd[1226]: time="2024-01-01T11:44:33.418039652Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jan 01 11:44:51 minikube cri-dockerd[1111]: time="2024-01-01T11:44:51Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Jan 01 11:44:52 minikube dockerd[1226]: time="2024-01-01T11:44:52.356037567Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Jan 01 11:44:52 minikube dockerd[1226]: time="2024-01-01T11:44:52.356148411Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jan 01 11:44:52 minikube dockerd[1226]: time="2024-01-01T11:44:52.356176806Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Jan 01 11:44:52 minikube dockerd[1226]: time="2024-01-01T11:44:52.356191763Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jan 01 11:44:52 minikube dockerd[1226]: time="2024-01-01T11:44:52.362211887Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Jan 01 11:44:52 minikube dockerd[1226]: time="2024-01-01T11:44:52.362286962Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jan 01 11:44:52 minikube dockerd[1226]: time="2024-01-01T11:44:52.362313782Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Jan 01 11:44:52 minikube dockerd[1226]: time="2024-01-01T11:44:52.362324572Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jan 01 11:44:52 minikube cri-dockerd[1111]: time="2024-01-01T11:44:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a3cb3ea8b9b2a00e41a819b784a3bfae68ccfdf52004b374b76a745c338dedb0/resolv.conf as [nameserver 192.168.64.1]"
Jan 01 11:44:52 minikube dockerd[1226]: time="2024-01-01T11:44:52.499401058Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Jan 01 11:44:52 minikube dockerd[1226]: time="2024-01-01T11:44:52.499478462Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jan 01 11:44:52 minikube dockerd[1226]: time="2024-01-01T11:44:52.499496848Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Jan 01 11:44:52 minikube dockerd[1226]: time="2024-01-01T11:44:52.499506670Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jan 01 11:44:55 minikube cri-dockerd[1111]: time="2024-01-01T11:44:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/27859e0b84143921d594b08fc9d5fc6b3ce5f167cd60967fa747e4ededd4b7b8/resolv.conf as [nameserver 192.168.64.1]"
Jan 01 11:45:04 minikube cri-dockerd[1111]: time="2024-01-01T11:45:04Z" level=info msg="Stop pulling image docker.io/kindest/kindnetd:v20230809-80a64d96: Status: Downloaded newer image for kindest/kindnetd:v20230809-80a64d96"
Jan 01 11:45:04 minikube dockerd[1226]: time="2024-01-01T11:45:04.904032478Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Jan 01 11:45:04 minikube dockerd[1226]: time="2024-01-01T11:45:04.904112158Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jan 01 11:45:04 minikube dockerd[1226]: time="2024-01-01T11:45:04.904127956Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Jan 01 11:45:04 minikube dockerd[1226]: time="2024-01-01T11:45:04.904137557Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jan 01 11:45:09 minikube dockerd[1226]: time="2024-01-01T11:45:09.939082634Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Jan 01 11:45:09 minikube dockerd[1226]: time="2024-01-01T11:45:09.939227673Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jan 01 11:45:09 minikube dockerd[1226]: time="2024-01-01T11:45:09.939265216Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Jan 01 11:45:09 minikube dockerd[1226]: time="2024-01-01T11:45:09.939282845Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jan 01 11:45:09 minikube dockerd[1226]: time="2024-01-01T11:45:09.955412875Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Jan 01 11:45:09 minikube dockerd[1226]: time="2024-01-01T11:45:09.955543128Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jan 01 11:45:09 minikube dockerd[1226]: time="2024-01-01T11:45:09.955792651Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Jan 01 11:45:09 minikube dockerd[1226]: time="2024-01-01T11:45:09.955803934Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jan 01 11:45:10 minikube cri-dockerd[1111]: time="2024-01-01T11:45:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a02c089168957fc79378b52f7422e38ec9766420ce7b566030e03e54233b3623/resolv.conf as [nameserver 192.168.64.1]"
Jan 01 11:45:10 minikube cri-dockerd[1111]: time="2024-01-01T11:45:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bd5ece6a3951c3a202e7b3717306368c7cac463657728f11f40f7c48d069aebc/resolv.conf as [nameserver 192.168.64.1]"
Jan 01 11:45:10 minikube dockerd[1226]: time="2024-01-01T11:45:10.517094127Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Jan 01 11:45:10 minikube dockerd[1226]: time="2024-01-01T11:45:10.517231310Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jan 01 11:45:10 minikube dockerd[1226]: time="2024-01-01T11:45:10.517296703Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Jan 01 11:45:10 minikube dockerd[1226]: time="2024-01-01T11:45:10.517311525Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jan 01 11:45:10 minikube dockerd[1226]: time="2024-01-01T11:45:10.565227519Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Jan 01 11:45:10 minikube dockerd[1226]: time="2024-01-01T11:45:10.566064897Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Jan 01 11:45:10 minikube dockerd[1226]: time="2024-01-01T11:45:10.566303955Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Jan 01 11:45:10 minikube dockerd[1226]: time="2024-01-01T11:45:10.566366547Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                      CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
285d2b0096307       ead0a4a53df89                                                                              10 seconds ago      Running             coredns                   0                   bd5ece6a3951c       coredns-5dd5756b68-4qf6j
bc1e9ee102d98       6e38f40d628db                                                                              10 seconds ago      Running             storage-provisioner       0                   a02c089168957       storage-provisioner
bd8f92a51d356       kindest/kindnetd@sha256:4a58d1cd2b45bf2460762a51a4aa9c80861f460af35800c05baab0573f923052   16 seconds ago      Running             kindnet-cni               0                   27859e0b84143       kindnet-l89bf
807f8b0b8ab69       bfc896cf80fba                                                                              28 seconds ago      Running             kube-proxy                0                   a3cb3ea8b9b2a       kube-proxy-9fhdx
33ee2c13fbdad       6d1b4fd1b182d                                                                              47 seconds ago      Running             kube-scheduler            0                   7b6b005ba81ee       kube-scheduler-minikube
c88189c77e6a1       73deb9a3f7025                                                                              47 seconds ago      Running             etcd                      0                   120d5ffee4215       etcd-minikube
fba93635b34fe       10baa1ca17068                                                                              48 seconds ago      Running             kube-controller-manager   0                   7e81f325d78bb       kube-controller-manager-minikube
607e20c788a04       5374347291230                                                                              48 seconds ago      Running             kube-apiserver            0                   6bbb9873725dc       kube-apiserver-minikube

* 
* ==> coredns [285d2b009630] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = 82b95b61957b89eeea31bdaf6987f010031330ef97d5f8469dbdaa80b119a5b0c9955b961009dd5b77ee3ada002b456836be781510516cbd9d015b1a704a24ea
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:33725 - 56318 "HINFO IN 983695803565300492.1424595904775506982. udp 56 false 512" NXDOMAIN qr,rd,ra 131 0.028147735s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_01_01T15_44_40_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 01 Jan 2024 11:44:35 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 01 Jan 2024 11:45:20 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 01 Jan 2024 11:45:09 +0000   Mon, 01 Jan 2024 11:44:33 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 01 Jan 2024 11:45:09 +0000   Mon, 01 Jan 2024 11:44:33 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 01 Jan 2024 11:45:09 +0000   Mon, 01 Jan 2024 11:44:33 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 01 Jan 2024 11:45:09 +0000   Mon, 01 Jan 2024 11:45:09 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.64.11
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  17784752Ki
  hugepages-2Mi:      0
  memory:             8135584Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  17784752Ki
  hugepages-2Mi:      0
  memory:             8135584Ki
  pods:               110
System Info:
  Machine ID:                 5badbf0ab6044779b6d8e3778a3905b9
  System UUID:                0eae11ee-0000-0000-8a75-acde48001122
  Boot ID:                    770addb4-146d-49cd-bf42-19a81bf0c0df
  Kernel Version:             5.10.57
  OS Image:                   Buildroot 2021.02.12
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-5dd5756b68-4qf6j            100m (5%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     28s
  kube-system                 etcd-minikube                       100m (5%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         41s
  kube-system                 kindnet-l89bf                       100m (5%!)(MISSING)     100m (5%!)(MISSING)   50Mi (0%!)(MISSING)        50Mi (0%!)(MISSING)      29s
  kube-system                 kube-apiserver-minikube             250m (12%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         44s
  kube-system                 kube-controller-manager-minikube    200m (10%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         41s
  kube-system                 kube-proxy-9fhdx                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         29s
  kube-system                 kube-scheduler-minikube             100m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         41s
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         40s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (42%!)(MISSING)  100m (5%!)(MISSING)
  memory             220Mi (2%!)(MISSING)  220Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 28s                kube-proxy       
  Normal  Starting                 49s                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  49s (x8 over 49s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    49s (x8 over 49s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     49s (x7 over 49s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  49s                kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 42s                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  42s                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  42s                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    42s                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     42s                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           29s                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  NodeReady                11s                kubelet          Node minikube status is now: NodeReady

* 
* ==> dmesg <==
* [Jan 1 11:44] ERROR: earlyprintk= earlyser already used
[  +0.000000] You have booted with nomodeset. This means your GPU drivers are DISABLED
[  +0.000001] Any video related functionality will be severely degraded, and you may not even be able to suspend the system properly
[  +0.000000] Unless you actually understand what nomodeset does, you should reboot without enabling it
[  +0.167505] ACPI BIOS Warning (bug): Incorrect checksum in table [DSDT] - 0xBE, should be 0x1B (20200925/tbprint-173)
[  +5.918152] ACPI Error: Could not enable RealTimeClock event (20200925/evxfevnt-182)
[  +0.000003] ACPI Warning: Could not enable fixed event - RealTimeClock (4) (20200925/evxface-618)
[  +0.011807] platform regulatory.0: Direct firmware load for regulatory.db failed with error -2
[  +3.556019] systemd-fstab-generator[124]: Ignoring "noauto" for root device
[  +0.045864] systemd[1]: systemd-journald.service: unit configures an IP firewall, but the local system does not support BPF/cgroup firewalling.
[  +0.000002] systemd[1]: (This warning is only shown for the first unit using IP firewalling.)
[  +1.952061] NFSD: Using /var/lib/nfs/v4recovery as the NFSv4 state recovery directory
[  +0.000005] NFSD: unable to find recovery directory /var/lib/nfs/v4recovery
[  +0.000001] NFSD: Unable to initialize client recovery tracking! (-2)
[  +0.543809] systemd-fstab-generator[536]: Ignoring "noauto" for root device
[  +0.092863] systemd-fstab-generator[547]: Ignoring "noauto" for root device
[  +1.333539] systemd-fstab-generator[804]: Ignoring "noauto" for root device
[  +0.255169] systemd-fstab-generator[842]: Ignoring "noauto" for root device
[  +0.095359] systemd-fstab-generator[853]: Ignoring "noauto" for root device
[  +0.112268] systemd-fstab-generator[866]: Ignoring "noauto" for root device
[  +1.411712] systemd-fstab-generator[1024]: Ignoring "noauto" for root device
[  +0.094038] systemd-fstab-generator[1035]: Ignoring "noauto" for root device
[  +0.092917] systemd-fstab-generator[1047]: Ignoring "noauto" for root device
[  +0.106502] systemd-fstab-generator[1058]: Ignoring "noauto" for root device
[  +0.106921] systemd-fstab-generator[1077]: Ignoring "noauto" for root device
[  +6.455546] systemd-fstab-generator[1211]: Ignoring "noauto" for root device
[  +2.102644] kauditd_printk_skb: 55 callbacks suppressed
[  +4.029167] systemd-fstab-generator[1579]: Ignoring "noauto" for root device
[  +7.283037] systemd-fstab-generator[2469]: Ignoring "noauto" for root device
[ +13.490536] kauditd_printk_skb: 39 callbacks suppressed
[Jan 1 11:45] kauditd_printk_skb: 14 callbacks suppressed

* 
* ==> etcd [c88189c77e6a] <==
* {"level":"warn","ts":"2024-01-01T11:44:33.582982Z","caller":"embed/config.go:673","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-01-01T11:44:33.585571Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.64.11:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.64.11:2380","--initial-cluster=minikube=https://192.168.64.11:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.64.11:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.64.11:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2024-01-01T11:44:33.586194Z","caller":"embed/config.go:673","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-01-01T11:44:33.586417Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.64.11:2380"]}
{"level":"info","ts":"2024-01-01T11:44:33.586569Z","caller":"embed/etcd.go:495","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-01-01T11:44:33.588268Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.64.11:2379"]}
{"level":"info","ts":"2024-01-01T11:44:33.590512Z","caller":"embed/etcd.go:309","msg":"starting an etcd server","etcd-version":"3.5.9","git-sha":"bdbbde998","go-version":"go1.19.9","go-os":"linux","go-arch":"amd64","max-cpu-set":2,"max-cpu-available":2,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.64.11:2380"],"listen-peer-urls":["https://192.168.64.11:2380"],"advertise-client-urls":["https://192.168.64.11:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.64.11:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.64.11:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-01-01T11:44:33.593567Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"2.630538ms"}
{"level":"info","ts":"2024-01-01T11:44:33.620296Z","caller":"etcdserver/raft.go:495","msg":"starting local member","local-member-id":"4645e31c6601cdb4","cluster-id":"d4f31da6fd6eeac6"}
{"level":"info","ts":"2024-01-01T11:44:33.620574Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"4645e31c6601cdb4 switched to configuration voters=()"}
{"level":"info","ts":"2024-01-01T11:44:33.620715Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"4645e31c6601cdb4 became follower at term 0"}
{"level":"info","ts":"2024-01-01T11:44:33.620854Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft 4645e31c6601cdb4 [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2024-01-01T11:44:33.620955Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"4645e31c6601cdb4 became follower at term 1"}
{"level":"info","ts":"2024-01-01T11:44:33.621024Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"4645e31c6601cdb4 switched to configuration voters=(5063703067157974452)"}
{"level":"warn","ts":"2024-01-01T11:44:33.631276Z","caller":"auth/store.go:1238","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-01-01T11:44:33.636551Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2024-01-01T11:44:33.650404Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-01-01T11:44:33.658623Z","caller":"etcdserver/server.go:854","msg":"starting etcd server","local-member-id":"4645e31c6601cdb4","local-server-version":"3.5.9","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-01-01T11:44:33.664664Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"4645e31c6601cdb4","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-01-01T11:44:33.671009Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-01T11:44:33.672914Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-01T11:44:33.672584Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"4645e31c6601cdb4 switched to configuration voters=(5063703067157974452)"}
{"level":"info","ts":"2024-01-01T11:44:33.677899Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"d4f31da6fd6eeac6","local-member-id":"4645e31c6601cdb4","added-peer-id":"4645e31c6601cdb4","added-peer-peer-urls":["https://192.168.64.11:2380"]}
{"level":"info","ts":"2024-01-01T11:44:33.678212Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-01T11:44:33.679189Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-01-01T11:44:33.682591Z","caller":"embed/etcd.go:278","msg":"now serving peer/client/metrics","local-member-id":"4645e31c6601cdb4","initial-advertise-peer-urls":["https://192.168.64.11:2380"],"listen-peer-urls":["https://192.168.64.11:2380"],"advertise-client-urls":["https://192.168.64.11:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.64.11:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-01-01T11:44:33.682635Z","caller":"embed/etcd.go:855","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-01-01T11:44:33.679304Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.64.11:2380"}
{"level":"info","ts":"2024-01-01T11:44:33.683347Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.64.11:2380"}
{"level":"info","ts":"2024-01-01T11:44:34.123354Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"4645e31c6601cdb4 is starting a new election at term 1"}
{"level":"info","ts":"2024-01-01T11:44:34.12341Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"4645e31c6601cdb4 became pre-candidate at term 1"}
{"level":"info","ts":"2024-01-01T11:44:34.123426Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"4645e31c6601cdb4 received MsgPreVoteResp from 4645e31c6601cdb4 at term 1"}
{"level":"info","ts":"2024-01-01T11:44:34.123435Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"4645e31c6601cdb4 became candidate at term 2"}
{"level":"info","ts":"2024-01-01T11:44:34.123439Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"4645e31c6601cdb4 received MsgVoteResp from 4645e31c6601cdb4 at term 2"}
{"level":"info","ts":"2024-01-01T11:44:34.123445Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"4645e31c6601cdb4 became leader at term 2"}
{"level":"info","ts":"2024-01-01T11:44:34.123821Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: 4645e31c6601cdb4 elected leader 4645e31c6601cdb4 at term 2"}
{"level":"info","ts":"2024-01-01T11:44:34.130787Z","caller":"etcdserver/server.go:2571","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2024-01-01T11:44:34.131181Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"4645e31c6601cdb4","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.64.11:2379]}","request-path":"/0/members/4645e31c6601cdb4/attributes","cluster-id":"d4f31da6fd6eeac6","publish-timeout":"7s"}
{"level":"info","ts":"2024-01-01T11:44:34.131489Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-01-01T11:44:34.131498Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-01-01T11:44:34.135455Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-01-01T11:44:34.131551Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-01-01T11:44:34.135882Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-01-01T11:44:34.141588Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.64.11:2379"}
{"level":"info","ts":"2024-01-01T11:44:34.145936Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"d4f31da6fd6eeac6","local-member-id":"4645e31c6601cdb4","cluster-version":"3.5"}
{"level":"info","ts":"2024-01-01T11:44:34.187061Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-01-01T11:44:34.197149Z","caller":"etcdserver/server.go:2595","msg":"cluster version is updated","cluster-version":"3.5"}

* 
* ==> kernel <==
*  11:45:21 up 1 min,  0 users,  load average: 1.14, 0.39, 0.14
Linux minikube 5.10.57 #1 SMP Tue Nov 7 06:51:54 UTC 2023 x86_64 GNU/Linux
PRETTY_NAME="Buildroot 2021.02.12"

* 
* ==> kindnet [bd8f92a51d35] <==
* I0101 11:45:05.185015       1 main.go:102] connected to apiserver: https://10.96.0.1:443
I0101 11:45:05.185264       1 main.go:107] hostIP = 192.168.64.11
podIP = 192.168.64.11
I0101 11:45:05.185489       1 main.go:116] setting mtu 1500 for CNI 
I0101 11:45:05.185523       1 main.go:146] kindnetd IP family: "ipv4"
I0101 11:45:05.185613       1 main.go:150] noMask IPv4 subnets: [10.244.0.0/16]
I0101 11:45:06.078298       1 main.go:223] Handling node with IPs: map[192.168.64.11:{}]
I0101 11:45:06.078357       1 main.go:227] handling current node
I0101 11:45:16.088658       1 main.go:223] Handling node with IPs: map[192.168.64.11:{}]
I0101 11:45:16.088753       1 main.go:227] handling current node

* 
* ==> kube-apiserver [607e20c788a0] <==
* I0101 11:44:35.538779       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0101 11:44:35.539458       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0101 11:44:35.539536       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0101 11:44:35.539632       1 controller.go:78] Starting OpenAPI AggregationController
I0101 11:44:35.539889       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0101 11:44:35.539980       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0101 11:44:35.540076       1 apf_controller.go:372] Starting API Priority and Fairness config controller
I0101 11:44:35.540246       1 handler_discovery.go:412] Starting ResourceDiscoveryManager
I0101 11:44:35.540383       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0101 11:44:35.540413       1 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0101 11:44:35.540493       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0101 11:44:35.540533       1 available_controller.go:423] Starting AvailableConditionController
I0101 11:44:35.540537       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0101 11:44:35.540957       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0101 11:44:35.543616       1 controller.go:134] Starting OpenAPI controller
I0101 11:44:35.543845       1 controller.go:85] Starting OpenAPI V3 controller
I0101 11:44:35.543946       1 naming_controller.go:291] Starting NamingConditionController
I0101 11:44:35.544085       1 establishing_controller.go:76] Starting EstablishingController
I0101 11:44:35.544169       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0101 11:44:35.544215       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0101 11:44:35.544306       1 crd_finalizer.go:266] Starting CRDFinalizer
I0101 11:44:35.540047       1 aggregator.go:164] waiting for initial CRD sync...
I0101 11:44:35.540056       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0101 11:44:35.540272       1 controller.go:116] Starting legacy_token_tracking_controller
I0101 11:44:35.545029       1 shared_informer.go:311] Waiting for caches to sync for configmaps
I0101 11:44:35.540288       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0101 11:44:35.578779       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0101 11:44:35.580808       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0101 11:44:35.581124       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0101 11:44:35.586859       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0101 11:44:35.740857       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0101 11:44:35.741075       1 apf_controller.go:377] Running API Priority and Fairness config worker
I0101 11:44:35.741101       1 apf_controller.go:380] Running API Priority and Fairness periodic rebalancing process
I0101 11:44:35.741183       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0101 11:44:35.742323       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0101 11:44:35.745613       1 shared_informer.go:318] Caches are synced for configmaps
I0101 11:44:35.749042       1 controller.go:624] quota admission added evaluator for: namespaces
E0101 11:44:35.766390       1 controller.go:146] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0101 11:44:35.781158       1 shared_informer.go:318] Caches are synced for crd-autoregister
I0101 11:44:35.781196       1 aggregator.go:166] initial CRD sync complete...
I0101 11:44:35.781203       1 autoregister_controller.go:141] Starting autoregister controller
I0101 11:44:35.781208       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0101 11:44:35.781213       1 cache.go:39] Caches are synced for autoregister controller
I0101 11:44:35.789457       1 shared_informer.go:318] Caches are synced for node_authorizer
I0101 11:44:35.970038       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I0101 11:44:36.549815       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0101 11:44:36.557257       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0101 11:44:36.557721       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0101 11:44:37.115489       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0101 11:44:37.149963       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0101 11:44:37.288397       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0101 11:44:37.296076       1 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.64.11]
I0101 11:44:37.297757       1 controller.go:624] quota admission added evaluator for: endpoints
I0101 11:44:37.301976       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0101 11:44:37.615546       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I0101 11:44:38.742920       1 controller.go:624] quota admission added evaluator for: deployments.apps
I0101 11:44:38.754308       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0101 11:44:38.764761       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0101 11:44:51.647705       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I0101 11:44:51.946924       1 controller.go:624] quota admission added evaluator for: controllerrevisions.apps

* 
* ==> kube-controller-manager [fba93635b34f] <==
* I0101 11:44:51.007098       1 shared_informer.go:318] Caches are synced for namespace
I0101 11:44:51.012432       1 shared_informer.go:318] Caches are synced for stateful set
I0101 11:44:51.014625       1 shared_informer.go:318] Caches are synced for persistent volume
I0101 11:44:51.023384       1 shared_informer.go:318] Caches are synced for attach detach
I0101 11:44:51.028014       1 shared_informer.go:318] Caches are synced for ephemeral
I0101 11:44:51.035401       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0101 11:44:51.037907       1 shared_informer.go:318] Caches are synced for service account
I0101 11:44:51.038207       1 shared_informer.go:318] Caches are synced for expand
I0101 11:44:51.038233       1 shared_informer.go:318] Caches are synced for PVC protection
I0101 11:44:51.038246       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0101 11:44:51.038489       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0101 11:44:51.039772       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0101 11:44:51.040931       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0101 11:44:51.043933       1 shared_informer.go:318] Caches are synced for ReplicationController
I0101 11:44:51.045115       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0101 11:44:51.046479       1 shared_informer.go:318] Caches are synced for TTL
I0101 11:44:51.050938       1 shared_informer.go:318] Caches are synced for node
I0101 11:44:51.050969       1 range_allocator.go:174] "Sending events to api server"
I0101 11:44:51.051185       1 range_allocator.go:178] "Starting range CIDR allocator"
I0101 11:44:51.051192       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0101 11:44:51.051197       1 shared_informer.go:318] Caches are synced for cidrallocator
I0101 11:44:51.051520       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0101 11:44:51.057187       1 range_allocator.go:380] "Set node PodCIDR" node="minikube" podCIDRs=["10.244.0.0/24"]
I0101 11:44:51.064365       1 shared_informer.go:318] Caches are synced for TTL after finished
I0101 11:44:51.070790       1 shared_informer.go:318] Caches are synced for GC
I0101 11:44:51.089928       1 shared_informer.go:318] Caches are synced for disruption
I0101 11:44:51.095697       1 shared_informer.go:318] Caches are synced for HPA
I0101 11:44:51.095822       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0101 11:44:51.096496       1 shared_informer.go:318] Caches are synced for endpoint
I0101 11:44:51.164529       1 shared_informer.go:318] Caches are synced for resource quota
I0101 11:44:51.170669       1 shared_informer.go:318] Caches are synced for resource quota
I0101 11:44:51.175105       1 shared_informer.go:318] Caches are synced for taint
I0101 11:44:51.175225       1 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0101 11:44:51.175361       1 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0101 11:44:51.175423       1 node_lifecycle_controller.go:1029] "Controller detected that all Nodes are not-Ready. Entering master disruption mode"
I0101 11:44:51.175564       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0101 11:44:51.175719       1 taint_manager.go:211] "Sending events to api server"
I0101 11:44:51.176316       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0101 11:44:51.187310       1 event.go:307] "Event occurred" object="kube-system/etcd-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0101 11:44:51.190750       1 shared_informer.go:318] Caches are synced for daemon sets
I0101 11:44:51.198740       1 event.go:307] "Event occurred" object="kube-system/kube-scheduler-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0101 11:44:51.199474       1 event.go:307] "Event occurred" object="kube-system/kube-apiserver-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0101 11:44:51.199513       1 event.go:307] "Event occurred" object="kube-system/kube-controller-manager-minikube" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0101 11:44:51.581752       1 shared_informer.go:318] Caches are synced for garbage collector
I0101 11:44:51.640499       1 shared_informer.go:318] Caches are synced for garbage collector
I0101 11:44:51.640540       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0101 11:44:51.651394       1 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-5dd5756b68 to 1"
I0101 11:44:51.956125       1 event.go:307] "Event occurred" object="kube-system/kindnet" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kindnet-l89bf"
I0101 11:44:51.960416       1 event.go:307] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-9fhdx"
I0101 11:44:52.060553       1 event.go:307] "Event occurred" object="kube-system/coredns-5dd5756b68" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-5dd5756b68-4qf6j"
I0101 11:44:52.071569       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="419.544042ms"
I0101 11:44:52.080658       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="9.051844ms"
I0101 11:44:52.080732       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="26.095¬µs"
I0101 11:44:52.080781       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="32.654¬µs"
I0101 11:45:09.521319       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="41.227¬µs"
I0101 11:45:09.541412       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="66.718¬µs"
I0101 11:45:11.183406       1 node_lifecycle_controller.go:1048] "Controller detected that some Nodes are Ready. Exiting master disruption mode"
I0101 11:45:11.655421       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="92.598¬µs"
I0101 11:45:11.683941       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="13.099225ms"
I0101 11:45:11.684018       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="25.29¬µs"

* 
* ==> kube-proxy [807f8b0b8ab6] <==
* I0101 11:44:52.611526       1 server_others.go:69] "Using iptables proxy"
I0101 11:44:52.619261       1 node.go:141] Successfully retrieved node IP: 192.168.64.11
I0101 11:44:52.649179       1 server_others.go:121] "No iptables support for family" ipFamily="IPv6"
I0101 11:44:52.649200       1 server.go:634] "kube-proxy running in single-stack mode" ipFamily="IPv4"
I0101 11:44:52.652629       1 server_others.go:152] "Using iptables Proxier"
I0101 11:44:52.652685       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0101 11:44:52.652903       1 server.go:846] "Version info" version="v1.28.3"
I0101 11:44:52.653255       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0101 11:44:52.654119       1 config.go:188] "Starting service config controller"
I0101 11:44:52.654179       1 shared_informer.go:311] Waiting for caches to sync for service config
I0101 11:44:52.654199       1 config.go:97] "Starting endpoint slice config controller"
I0101 11:44:52.654203       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0101 11:44:52.654674       1 config.go:315] "Starting node config controller"
I0101 11:44:52.654705       1 shared_informer.go:311] Waiting for caches to sync for node config
I0101 11:44:52.754266       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0101 11:44:52.754345       1 shared_informer.go:318] Caches are synced for service config
I0101 11:44:52.754746       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-scheduler [33ee2c13fbda] <==
* I0101 11:44:35.646768       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0101 11:44:35.655312       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0101 11:44:35.655488       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0101 11:44:35.655497       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0101 11:44:35.655507       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W0101 11:44:35.666193       1 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0101 11:44:35.666290       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0101 11:44:35.666465       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0101 11:44:35.666534       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0101 11:44:35.666604       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0101 11:44:35.666697       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0101 11:44:35.666831       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0101 11:44:35.667061       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0101 11:44:35.667164       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0101 11:44:35.667252       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0101 11:44:35.667411       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0101 11:44:35.667475       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0101 11:44:35.671746       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0101 11:44:35.672084       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0101 11:44:35.672153       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0101 11:44:35.672268       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0101 11:44:35.672426       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0101 11:44:35.674216       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0101 11:44:35.672467       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0101 11:44:35.674231       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0101 11:44:35.672586       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0101 11:44:35.674243       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0101 11:44:35.672664       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0101 11:44:35.677327       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0101 11:44:35.672692       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0101 11:44:35.677350       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0101 11:44:35.673123       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0101 11:44:35.677360       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0101 11:44:35.673220       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0101 11:44:35.677454       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0101 11:44:36.542310       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0101 11:44:36.542341       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0101 11:44:36.660358       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0101 11:44:36.660405       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0101 11:44:36.680220       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0101 11:44:36.680329       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0101 11:44:36.702985       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0101 11:44:36.703029       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0101 11:44:36.781549       1 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0101 11:44:36.781642       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0101 11:44:36.811615       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0101 11:44:36.811723       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0101 11:44:36.818554       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0101 11:44:36.818602       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0101 11:44:36.819885       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0101 11:44:36.819963       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0101 11:44:36.835571       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0101 11:44:36.836051       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0101 11:44:36.837951       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0101 11:44:36.838098       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0101 11:44:36.945887       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0101 11:44:36.946004       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0101 11:44:36.961238       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0101 11:44:36.961357       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
I0101 11:44:39.456244       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* -- Journal begins at Mon 2024-01-01 11:44:12 UTC, ends at Mon 2024-01-01 11:45:21 UTC. --
Jan 01 11:44:38 minikube kubelet[2483]: I0101 11:44:38.973680    2483 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
Jan 01 11:44:38 minikube kubelet[2483]: I0101 11:44:38.973695    2483 state_mem.go:36] "Initialized new in-memory state store"
Jan 01 11:44:38 minikube kubelet[2483]: I0101 11:44:38.973860    2483 state_mem.go:88] "Updated default CPUSet" cpuSet=""
Jan 01 11:44:38 minikube kubelet[2483]: I0101 11:44:38.973875    2483 state_mem.go:96] "Updated CPUSet assignments" assignments={}
Jan 01 11:44:38 minikube kubelet[2483]: I0101 11:44:38.973880    2483 policy_none.go:49] "None policy: Start"
Jan 01 11:44:38 minikube kubelet[2483]: I0101 11:44:38.977994    2483 memory_manager.go:169] "Starting memorymanager" policy="None"
Jan 01 11:44:38 minikube kubelet[2483]: I0101 11:44:38.978125    2483 state_mem.go:35] "Initializing new in-memory state store"
Jan 01 11:44:38 minikube kubelet[2483]: I0101 11:44:38.978268    2483 state_mem.go:75] "Updated machine memory state"
Jan 01 11:44:38 minikube kubelet[2483]: I0101 11:44:38.981175    2483 manager.go:471] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
Jan 01 11:44:38 minikube kubelet[2483]: I0101 11:44:38.985594    2483 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
Jan 01 11:44:38 minikube kubelet[2483]: I0101 11:44:38.999650    2483 kubelet_node_status.go:70] "Attempting to register node" node="minikube"
Jan 01 11:44:39 minikube kubelet[2483]: I0101 11:44:39.016532    2483 kubelet_node_status.go:108] "Node was previously registered" node="minikube"
Jan 01 11:44:39 minikube kubelet[2483]: I0101 11:44:39.016686    2483 kubelet_node_status.go:73] "Successfully registered node" node="minikube"
Jan 01 11:44:39 minikube kubelet[2483]: I0101 11:44:39.045888    2483 topology_manager.go:215] "Topology Admit Handler" podUID="75ac196d3709dde303d8a81c035c2c28" podNamespace="kube-system" podName="kube-scheduler-minikube"
Jan 01 11:44:39 minikube kubelet[2483]: I0101 11:44:39.045982    2483 topology_manager.go:215] "Topology Admit Handler" podUID="60ec37118d1ed9e527de8032074e0a6e" podNamespace="kube-system" podName="etcd-minikube"
Jan 01 11:44:39 minikube kubelet[2483]: I0101 11:44:39.046013    2483 topology_manager.go:215] "Topology Admit Handler" podUID="4da89cba6dcedde2554f40eedef6171f" podNamespace="kube-system" podName="kube-apiserver-minikube"
Jan 01 11:44:39 minikube kubelet[2483]: I0101 11:44:39.047792    2483 topology_manager.go:215] "Topology Admit Handler" podUID="11fc41667a2819cdb15b7270cb5cd200" podNamespace="kube-system" podName="kube-controller-manager-minikube"
Jan 01 11:44:39 minikube kubelet[2483]: E0101 11:44:39.063619    2483 kubelet.go:1890] "Failed creating a mirror pod for" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Jan 01 11:44:39 minikube kubelet[2483]: I0101 11:44:39.081786    2483 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/75ac196d3709dde303d8a81c035c2c28-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"75ac196d3709dde303d8a81c035c2c28\") " pod="kube-system/kube-scheduler-minikube"
Jan 01 11:44:39 minikube kubelet[2483]: I0101 11:44:39.082013    2483 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/60ec37118d1ed9e527de8032074e0a6e-etcd-data\") pod \"etcd-minikube\" (UID: \"60ec37118d1ed9e527de8032074e0a6e\") " pod="kube-system/etcd-minikube"
Jan 01 11:44:39 minikube kubelet[2483]: I0101 11:44:39.082216    2483 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/4da89cba6dcedde2554f40eedef6171f-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"4da89cba6dcedde2554f40eedef6171f\") " pod="kube-system/kube-apiserver-minikube"
Jan 01 11:44:39 minikube kubelet[2483]: I0101 11:44:39.082350    2483 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/11fc41667a2819cdb15b7270cb5cd200-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"11fc41667a2819cdb15b7270cb5cd200\") " pod="kube-system/kube-controller-manager-minikube"
Jan 01 11:44:39 minikube kubelet[2483]: I0101 11:44:39.082454    2483 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/11fc41667a2819cdb15b7270cb5cd200-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"11fc41667a2819cdb15b7270cb5cd200\") " pod="kube-system/kube-controller-manager-minikube"
Jan 01 11:44:39 minikube kubelet[2483]: I0101 11:44:39.082507    2483 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/11fc41667a2819cdb15b7270cb5cd200-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"11fc41667a2819cdb15b7270cb5cd200\") " pod="kube-system/kube-controller-manager-minikube"
Jan 01 11:44:39 minikube kubelet[2483]: I0101 11:44:39.082650    2483 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/11fc41667a2819cdb15b7270cb5cd200-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"11fc41667a2819cdb15b7270cb5cd200\") " pod="kube-system/kube-controller-manager-minikube"
Jan 01 11:44:39 minikube kubelet[2483]: I0101 11:44:39.082762    2483 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/60ec37118d1ed9e527de8032074e0a6e-etcd-certs\") pod \"etcd-minikube\" (UID: \"60ec37118d1ed9e527de8032074e0a6e\") " pod="kube-system/etcd-minikube"
Jan 01 11:44:39 minikube kubelet[2483]: I0101 11:44:39.082873    2483 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/4da89cba6dcedde2554f40eedef6171f-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"4da89cba6dcedde2554f40eedef6171f\") " pod="kube-system/kube-apiserver-minikube"
Jan 01 11:44:39 minikube kubelet[2483]: I0101 11:44:39.082980    2483 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/4da89cba6dcedde2554f40eedef6171f-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"4da89cba6dcedde2554f40eedef6171f\") " pod="kube-system/kube-apiserver-minikube"
Jan 01 11:44:39 minikube kubelet[2483]: I0101 11:44:39.083108    2483 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/11fc41667a2819cdb15b7270cb5cd200-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"11fc41667a2819cdb15b7270cb5cd200\") " pod="kube-system/kube-controller-manager-minikube"
Jan 01 11:44:39 minikube kubelet[2483]: I0101 11:44:39.837639    2483 apiserver.go:52] "Watching apiserver"
Jan 01 11:44:39 minikube kubelet[2483]: I0101 11:44:39.879810    2483 desired_state_of_world_populator.go:159] "Finished populating initial desired state of world"
Jan 01 11:44:40 minikube kubelet[2483]: E0101 11:44:40.044223    2483 kubelet.go:1890] "Failed creating a mirror pod for" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Jan 01 11:44:40 minikube kubelet[2483]: I0101 11:44:40.068086    2483 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kube-scheduler-minikube" podStartSLOduration=1.068054253 podCreationTimestamp="2024-01-01 11:44:39 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-01-01 11:44:40.067669592 +0000 UTC m=+1.340000469" watchObservedRunningTime="2024-01-01 11:44:40.068054253 +0000 UTC m=+1.340385130"
Jan 01 11:44:40 minikube kubelet[2483]: I0101 11:44:40.098145    2483 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kube-controller-manager-minikube" podStartSLOduration=1.098112206 podCreationTimestamp="2024-01-01 11:44:39 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-01-01 11:44:40.082268364 +0000 UTC m=+1.354599239" watchObservedRunningTime="2024-01-01 11:44:40.098112206 +0000 UTC m=+1.370443081"
Jan 01 11:44:40 minikube kubelet[2483]: I0101 11:44:40.114096    2483 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/etcd-minikube" podStartSLOduration=1.113962437 podCreationTimestamp="2024-01-01 11:44:39 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-01-01 11:44:40.098236714 +0000 UTC m=+1.370567591" watchObservedRunningTime="2024-01-01 11:44:40.113962437 +0000 UTC m=+1.386293319"
Jan 01 11:44:40 minikube kubelet[2483]: I0101 11:44:40.114195    2483 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kube-apiserver-minikube" podStartSLOduration=4.114180209 podCreationTimestamp="2024-01-01 11:44:36 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-01-01 11:44:40.11152199 +0000 UTC m=+1.383852867" watchObservedRunningTime="2024-01-01 11:44:40.114180209 +0000 UTC m=+1.386511086"
Jan 01 11:44:51 minikube kubelet[2483]: I0101 11:44:51.107765    2483 kuberuntime_manager.go:1523] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Jan 01 11:44:51 minikube kubelet[2483]: I0101 11:44:51.108772    2483 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Jan 01 11:44:51 minikube kubelet[2483]: I0101 11:44:51.968610    2483 topology_manager.go:215] "Topology Admit Handler" podUID="338ac5cd-921e-4f6f-8322-683044bd6c8b" podNamespace="kube-system" podName="kindnet-l89bf"
Jan 01 11:44:51 minikube kubelet[2483]: I0101 11:44:51.972359    2483 topology_manager.go:215] "Topology Admit Handler" podUID="40de2056-8ea0-4f23-a856-51c79c160d8e" podNamespace="kube-system" podName="kube-proxy-9fhdx"
Jan 01 11:44:52 minikube kubelet[2483]: I0101 11:44:52.052118    2483 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-qlvd5\" (UniqueName: \"kubernetes.io/projected/40de2056-8ea0-4f23-a856-51c79c160d8e-kube-api-access-qlvd5\") pod \"kube-proxy-9fhdx\" (UID: \"40de2056-8ea0-4f23-a856-51c79c160d8e\") " pod="kube-system/kube-proxy-9fhdx"
Jan 01 11:44:52 minikube kubelet[2483]: I0101 11:44:52.052222    2483 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/338ac5cd-921e-4f6f-8322-683044bd6c8b-xtables-lock\") pod \"kindnet-l89bf\" (UID: \"338ac5cd-921e-4f6f-8322-683044bd6c8b\") " pod="kube-system/kindnet-l89bf"
Jan 01 11:44:52 minikube kubelet[2483]: I0101 11:44:52.052241    2483 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-pnmjs\" (UniqueName: \"kubernetes.io/projected/338ac5cd-921e-4f6f-8322-683044bd6c8b-kube-api-access-pnmjs\") pod \"kindnet-l89bf\" (UID: \"338ac5cd-921e-4f6f-8322-683044bd6c8b\") " pod="kube-system/kindnet-l89bf"
Jan 01 11:44:52 minikube kubelet[2483]: I0101 11:44:52.052256    2483 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/40de2056-8ea0-4f23-a856-51c79c160d8e-xtables-lock\") pod \"kube-proxy-9fhdx\" (UID: \"40de2056-8ea0-4f23-a856-51c79c160d8e\") " pod="kube-system/kube-proxy-9fhdx"
Jan 01 11:44:52 minikube kubelet[2483]: I0101 11:44:52.052272    2483 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/40de2056-8ea0-4f23-a856-51c79c160d8e-lib-modules\") pod \"kube-proxy-9fhdx\" (UID: \"40de2056-8ea0-4f23-a856-51c79c160d8e\") " pod="kube-system/kube-proxy-9fhdx"
Jan 01 11:44:52 minikube kubelet[2483]: I0101 11:44:52.052337    2483 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/338ac5cd-921e-4f6f-8322-683044bd6c8b-lib-modules\") pod \"kindnet-l89bf\" (UID: \"338ac5cd-921e-4f6f-8322-683044bd6c8b\") " pod="kube-system/kindnet-l89bf"
Jan 01 11:44:52 minikube kubelet[2483]: I0101 11:44:52.052383    2483 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/40de2056-8ea0-4f23-a856-51c79c160d8e-kube-proxy\") pod \"kube-proxy-9fhdx\" (UID: \"40de2056-8ea0-4f23-a856-51c79c160d8e\") " pod="kube-system/kube-proxy-9fhdx"
Jan 01 11:44:52 minikube kubelet[2483]: I0101 11:44:52.052400    2483 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-cfg\" (UniqueName: \"kubernetes.io/host-path/338ac5cd-921e-4f6f-8322-683044bd6c8b-cni-cfg\") pod \"kindnet-l89bf\" (UID: \"338ac5cd-921e-4f6f-8322-683044bd6c8b\") " pod="kube-system/kindnet-l89bf"
Jan 01 11:44:55 minikube kubelet[2483]: I0101 11:44:55.307484    2483 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="27859e0b84143921d594b08fc9d5fc6b3ce5f167cd60967fa747e4ededd4b7b8"
Jan 01 11:44:59 minikube kubelet[2483]: I0101 11:44:59.556621    2483 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kube-proxy-9fhdx" podStartSLOduration=8.556585748 podCreationTimestamp="2024-01-01 11:44:51 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-01-01 11:44:52.687602245 +0000 UTC m=+13.383092722" watchObservedRunningTime="2024-01-01 11:44:59.556585748 +0000 UTC m=+20.252076233"
Jan 01 11:45:09 minikube kubelet[2483]: I0101 11:45:09.469849    2483 kubelet_node_status.go:493] "Fast updating node status as it just became ready"
Jan 01 11:45:09 minikube kubelet[2483]: I0101 11:45:09.520579    2483 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kindnet-l89bf" podStartSLOduration=9.057450274 podCreationTimestamp="2024-01-01 11:44:51 +0000 UTC" firstStartedPulling="2024-01-01 11:44:55.309233961 +0000 UTC m=+16.004724436" lastFinishedPulling="2024-01-01 11:45:04.772331971 +0000 UTC m=+25.467822444" observedRunningTime="2024-01-01 11:45:05.425515867 +0000 UTC m=+26.121006343" watchObservedRunningTime="2024-01-01 11:45:09.520548282 +0000 UTC m=+30.216038758"
Jan 01 11:45:09 minikube kubelet[2483]: I0101 11:45:09.520842    2483 topology_manager.go:215] "Topology Admit Handler" podUID="989cc363-a8ef-421a-a368-c9979fd3ceb5" podNamespace="kube-system" podName="coredns-5dd5756b68-4qf6j"
Jan 01 11:45:09 minikube kubelet[2483]: I0101 11:45:09.522326    2483 topology_manager.go:215] "Topology Admit Handler" podUID="ee60d92b-1ed8-4ccb-8680-d2306cd4e26e" podNamespace="kube-system" podName="storage-provisioner"
Jan 01 11:45:09 minikube kubelet[2483]: I0101 11:45:09.576856    2483 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/989cc363-a8ef-421a-a368-c9979fd3ceb5-config-volume\") pod \"coredns-5dd5756b68-4qf6j\" (UID: \"989cc363-a8ef-421a-a368-c9979fd3ceb5\") " pod="kube-system/coredns-5dd5756b68-4qf6j"
Jan 01 11:45:09 minikube kubelet[2483]: I0101 11:45:09.577008    2483 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-xwvxr\" (UniqueName: \"kubernetes.io/projected/ee60d92b-1ed8-4ccb-8680-d2306cd4e26e-kube-api-access-xwvxr\") pod \"storage-provisioner\" (UID: \"ee60d92b-1ed8-4ccb-8680-d2306cd4e26e\") " pod="kube-system/storage-provisioner"
Jan 01 11:45:09 minikube kubelet[2483]: I0101 11:45:09.577031    2483 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-vg4mb\" (UniqueName: \"kubernetes.io/projected/989cc363-a8ef-421a-a368-c9979fd3ceb5-kube-api-access-vg4mb\") pod \"coredns-5dd5756b68-4qf6j\" (UID: \"989cc363-a8ef-421a-a368-c9979fd3ceb5\") " pod="kube-system/coredns-5dd5756b68-4qf6j"
Jan 01 11:45:09 minikube kubelet[2483]: I0101 11:45:09.577074    2483 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/ee60d92b-1ed8-4ccb-8680-d2306cd4e26e-tmp\") pod \"storage-provisioner\" (UID: \"ee60d92b-1ed8-4ccb-8680-d2306cd4e26e\") " pod="kube-system/storage-provisioner"
Jan 01 11:45:11 minikube kubelet[2483]: I0101 11:45:11.661237    2483 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=31.661205846 podCreationTimestamp="2024-01-01 11:44:40 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-01-01 11:45:11.635312618 +0000 UTC m=+32.330803102" watchObservedRunningTime="2024-01-01 11:45:11.661205846 +0000 UTC m=+32.356696319"
Jan 01 11:45:11 minikube kubelet[2483]: I0101 11:45:11.675482    2483 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/coredns-5dd5756b68-4qf6j" podStartSLOduration=19.675351459 podCreationTimestamp="2024-01-01 11:44:52 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-01-01 11:45:11.661490453 +0000 UTC m=+32.356980930" watchObservedRunningTime="2024-01-01 11:45:11.675351459 +0000 UTC m=+32.370841936"

* 
* ==> storage-provisioner [bc1e9ee102d9] <==
* I0101 11:45:10.609236       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0101 11:45:10.625398       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0101 11:45:10.626309       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0101 11:45:10.637668       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0101 11:45:10.638105       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_24a132a7-3fca-4ca1-89fe-28e59ac05c1e!
I0101 11:45:10.637711       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"4a27da32-0cca-4879-a14e-5f006d17ffcf", APIVersion:"v1", ResourceVersion:"435", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_24a132a7-3fca-4ca1-89fe-28e59ac05c1e became leader
I0101 11:45:10.739048       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_24a132a7-3fca-4ca1-89fe-28e59ac05c1e!

